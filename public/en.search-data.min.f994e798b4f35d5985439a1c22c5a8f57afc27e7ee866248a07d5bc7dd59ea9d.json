[{"id":0,"href":"/docs/preliminaries/","title":"Preliminaries","section":"Docs","content":" Preliminaries # Lattices # A lattice is defined as a set of points in n-dimensional space with a periodic structure. A lattice can be represented by a set of linearly independent vectors commonly named the basis of the lattice. If \\(\\bold{b_1}, \u0026hellip;, \\bold{b_n}\\) denote basis vectors we can describe a lattice by: $$\\Lambda(\\bold{b_1}, ..., \\bold{b_n}) = \\{\\sum_{i=1}^{n}x_i\\bold{b_i}: x_i \\in \\mathbb{Z}\\}$$\nIn the rest of this work, we will especially consider q-ary lattices described by a basis \\(\\bold{B} \\in \\mathbb{Z}^{h\\times w}_q\\), in which coefficients are taken modulo q.\nThe Short Vector Problem # The security of lattice based constructions rely on the fact that finding the shortest non-zero vector in a lattice is a hard problem. One usually considers an approximation version of the SVP problem. Given a basis \\(\\bold{B}\\) of a lattice, the approximate SVP problem is the problem of finding a short lattice vector \\(\\bold{v}\\) such that \\(0 \u0026lt; \\lVert \\bold{v} \\rVert \\leq \\gamma\\lambda_1(\\Lambda(\\bold{B}))\\) where \\(\\lambda_1\\) denotes the shortest nonzero vector length and \\(\\gamma\\) is the approximation factor.\nIt is actually believed that no polynomial time algorithm can approximate such a lattice problem within polynomial factors. Furthermore, it is also believed that no polynomial time quantum algorithm can approximate such a lattice problem within polynomial factors.\nLattice reduction algorithms # Lattice reduction algorithms aim to transform a given basis of a lattice into a \u0026ldquo;reduced\u0026rdquo; basis, where the vectors are shorter and closer to being orthogonal. This reduction makes it easier to approximate solutions to hard lattice problems like the Shortest Vector Problem. In short, the quality of a basis can be improved to make the problem easier. The most prominent lattice reduction algorithms include:\nLLL Algorithm (Lenstra–Lenstra–Lovász): it produces a reduced basis in polynomial time, where the vectors are guaranteed to be within a known factor of the shortest vector, but doesn’t necessarily find the shortest vector, BKZ Algorithm (Block Korkine-Zolotarev): the BKZ algorithm is a generalization of the LLL algorithm and provides better reduction at the cost of higher computational complexity. By working in blocks of the lattice and applying LLL reduction to these blocks, BKZ achieves stronger approximations of the shortest vector, though it requires more computational resources. Sieving and Enumeration: algorithms that are designed to give exact solutions to the SVP problem. Security # The foundational belief of security in new lattice-based primitives stems from one key finding. Ajtai’s theorem connected the hardness of certain average-case problems to the difficulty of worst-case problems in lattices. Specifically, Ajtai demonstrated that for the Short Integer Solution (SIS) problem which we will define later, the average-case instances are at least as hard as the worst-case instances of the Shortest Vector Problem (SVP) on lattices. This means that if one could efficiently solve random instances of SIS, then they could also solve the worst-case SVP, a problem believed to be intractable even for quantum computers. Ajtai’s theorem provides a strong security guarantee for lattice-based cryptographic schemes by grounding their security in the hardness of well-studied lattice problems like SVP.\n"},{"id":1,"href":"/docs/lattice-reduction/","title":"Lattice reduction","section":"Docs","content":" Lattice reduction # Lattice reduction algorithms such as LLL and BKZ make iterative local improvements to a basis. This means that the global cost can be seen as two-folds: how costly is it to make the local improvements, which corresponds to solving an exact SVP problem and how costly is the global behavior of the algorithm. This section focuses on the global behavior of lattice reduction algorithms, while the next section (Cost models) will focus about solving local improvements.\nUseful quantities in lattices # Volume # In each lattice, we can define the volume of the lattice as the volume of its fundamental parallellepiped (the area delimited by the basis vectors). This quantity is an invariant of the lattice and does not depend on the basis chosen. This means that by applying Gram-Schmidt orthogonalization to any basis will give us an orthogonal basis from which we can approximate the volume of the lattice as\n$$ Vol(\\Lambda) = \\prod_{i=0}^{d-1} \\lVert \\bold{b_i}^*\\rVert $$where $\\bold{b_i}*$ are the orthogonal Gram-Schmidt vectors. It is also important to remember that $Vol(\\Lambda) = |Det(\\bold{B})|$, where $\\bold{B}$ is the basis matrix.\nThis invariant is conceptually important because it tells us that not all basis vectors can be small at the same time.\nGaussian heuristic # The gaussian heuristics predicts the number of lattice points inside any measurable body $\\mathcal{B} \\subset \\mathbb{R}^d$ is approximately $\\frac{Vol(\\mathcal{B})}{Vol(\\Lambda)}$. Applied to an euclidean d-ball, this would give that the length of the first vector is approximately\n$$\\lambda_1(\\Lambda) \\approx (\\frac{Vol(\\mathcal{B})}{Vol(\\Lambda)})^{\\frac{1}{d}} \\approx \\sqrt{\\frac{d}{2\\pi e}} Vol(\\Lambda)^{\\frac{1}{d}}$$ LLL algorithm # The Lenstra-Lenstra-Lovász (LLL) algorithm is an efficient polynomial-time algorithm in lattice theory that finds a \u0026ldquo;nearly orthogonal\u0026rdquo; basis for a given lattice. It aims to transform any arbitrary basis of a lattice into a reduced basis where the basis vectors are short and close to orthogonal following two conditions:\nSize reduction: $$1 \\leq j \u003c i \\leq n\\colon \\left|\\mu_{i,j}\\right|\\leq 0.5 \\text{ for } \\mu_{i,j} =\\frac{\\langle\\mathbf{b}_i,\\mathbf{b}^*_j\\rangle}{\\langle\\mathbf{b}^*_j,\\mathbf{b}^*_j\\rangle}$$ Lovász condition: For $k=2,\u0026hellip;,n$ $$\\delta \\Vert \\mathbf{b}^*_{k-1}\\Vert^2 \\leq \\Vert \\mathbf{b}^*_k\\Vert^2+ \\mu_{k,k-1}^2\\Vert\\mathbf{b}^*_{k-1}\\Vert^2$$ We say the basis is LLL-reduced if there exists a parameter $\\delta \\in (0.25, 1)$.\nWhile there exists some theorems bounding the worst cases of lattice reduction algorithms, they tend to perform better in practices. Reasoning about the behaviors of such algorithms has therefore become a game of heuristics and approximations. Typically, the vectors that are outputed by the LLL algorithm are said to follow the geometrie series assumption in their length. This assumption tells us that the shape after lattice reduction is a line with a flatter slope as lattice reduction gets stronger. The goal of lattice reduction algorithm can therefore be interpreted seen by watching a graph of the log-length of vectors after reductions. The overall goal being to flatten the line, leading to a small basis.\nBKZ algorithm # The Block Korkine-Zolotarev (BKZ) algorithm is a lattice reduction algorithm that generalizes the LLL algorithm to achieve stronger reduction properties. The BKZ algorithm is defined as a blockwise reduction algorithm that iteratively applies a form of lattice basis reduction to overlapping blocks of vectors within the basis. Assuming we have an SVP oracle, the BKZ algorithm is defined as follows:\nData: LLL-reduced basis B (pre-processed) and block size beta repeat until np changes for k in 0 to d-1 LLL on local projected block [k, ..., k+beta-1] v \u0026lt;-- SVP-Oracle(local projected block[k, ..., k+beta-1]) insert v into B end A BKZ-$\\beta$ reduced basis satisfies, for $\\epsilon \u0026gt; 0$:\n$$\\lVert \\bold{b_0} \\rVert \\leq \\sqrt{(1 + \\epsilon) \\gamma_{\\beta}}^\\frac{d-1}{\\beta - 1} Vol(\\Lambda(\\bold{B}))$$, where $\\gamma_\\beta = sup{\\lambda_1(\\Lambda)| \\Lambda \\in \\mathbb{R}^\\beta, Vol(\\Lambda) = 1}$\nGSA assumption and root hermite factor # "},{"id":2,"href":"/docs/cost-models/","title":"Cost Models","section":"Docs","content":" Cost Models # "},{"id":3,"href":"/docs/sis/","title":"SIS","section":"Docs","content":" Short Integer Solution problem (SIS) # Let us look at two different ways to define q-ary lattices. From \\(\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}\\)\n$$\\Lambda_q(\\bold{A^T}) = \\{\\bold{y} \\in \\mathbb{Z}^w : \\bold{y} = \\bold{A}^T\\bold{s} \\text{ mod q }\\text{ for some } \\bold{s} \\in \\mathbb{Z}^h\\}$$$$\\Lambda^T_q(\\bold{A}) = \\{\\bold{y} \\in \\mathbb{Z}^w : \\bold{A}\\bold{y} = \\bold{0} \\text{ mod q }\\}$$The first lattice is generally called the primal problem and relates to the LWE problem since finding a short vector in \\(\\Lambda_q(\\bold{A^T})\\) corresponds to solving the LWE problem. The second lattice is generally called the dual problem and relates to the SIS problem since finding a short vector in \\(\\Lambda^T_q(\\bold{A})\\) corresponds to solving the SIS problem.\nFormal definition # We define \\(SIS(h, q, w, \\beta)\\) as follows:\nGiven \\(\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}\\) find the short vector \\(\\bold{s} \\in \\mathbb{Z}^w\\) where \\(0 \u0026lt; \\lVert s \\rVert_p \\leq \\beta\\).\nWe can note that the problem becomes trivial as soon as \\(\\beta \\geq q\\), no matter the norm used.\nEstimating SIS hardness # Estimating the hardness of an SIS instance is done via estimating the number of operations required to run lattice reductions attacks on the related SVP problem. This concretely means that we need to know:\nHow small we expect the shortest vector to be in our SVP problem. How small we expect vectors to be after lattice reduction. How to cost lattice reduction. We will mostly focus on BKZ 2.0 as a reduction algorithm and we will present how we estimate all parts of the attack.\nExpected size of small vectors # Given a random matrix \\(\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}\\)\n"},{"id":4,"href":"/docs/sis/module-sis/","title":"Module Sis","section":"SIS","content":" Module-SIS # "},{"id":5,"href":"/docs/sis/ring-sis/","title":"Ring Sis","section":"SIS","content":" Ring-SIS # "}]