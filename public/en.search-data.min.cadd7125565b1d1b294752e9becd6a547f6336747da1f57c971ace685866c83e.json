[{"id":0,"href":"/docs/preliminaries/","title":"Preliminaries","section":"Docs","content":" Preliminaries # Lattices # A lattice is defined as a set of points in n-dimensional space with a periodic structure. A lattice can be represented by a set of linearly independent vectors commonly named the basis of the lattice. If \\(\\bold{b_1}, \u0026hellip;, \\bold{b_n}\\) denote basis vectors we can describe a lattice by: $$\\Lambda(\\bold{b_1}, ..., \\bold{b_n}) = \\{\\sum_{i=1}^{n}x_i\\bold{b_i}: x_i \\in \\mathbb{Z}\\}$$\nIn the rest of this work, we will especially consider q-ary lattices described by a basis \\(\\bold{B} \\in \\mathbb{Z}^{h\\times w}_q\\), in which coefficients are taken modulo q.\nThe Short Vector Problem # The security of lattice based constructions rely on the fact that finding the shortest non-zero vector in a lattice is a hard problem. One usually considers an approximation version of the SVP problem. Given a basis \\(\\bold{B}\\) of a lattice, the approximate SVP problem is the problem of finding a short lattice vector \\(\\bold{v}\\) such that \\(0 \u0026lt; \\lVert \\bold{v} \\rVert \\leq \\gamma\\lambda_1(\\Lambda(\\bold{B}))\\) where \\(\\lambda_1\\) denotes the shortest nonzero vector length and \\(\\gamma\\) is the approximation factor.\nIt is actually believed that no polynomial time algorithm can approximate such a lattice problem within polynomial factors. Furthermore, it is also believed that no polynomial time quantum algorithm can approximate such a lattice problem within polynomial factors.\nLattice reduction algorithms # Lattice reduction algorithms aim to transform a given basis of a lattice into a \u0026ldquo;reduced\u0026rdquo; basis, where the vectors are shorter and closer to being orthogonal. This reduction makes it easier to approximate solutions to hard lattice problems like the Shortest Vector Problem. In short, the quality of a basis can be improved to make the problem easier. The most prominent lattice reduction algorithms include:\nLLL Algorithm (Lenstra–Lenstra–Lovász): it produces a reduced basis in polynomial time, where the vectors are guaranteed to be within a known factor of the shortest vector, but doesn’t necessarily find the shortest vector, BKZ Algorithm (Block Korkine-Zolotarev): the BKZ algorithm is a generalization of the LLL algorithm and provides better reduction at the cost of higher computational complexity. By working in blocks of the lattice and applying LLL reduction to these blocks, BKZ achieves stronger approximations of the shortest vector, though it requires more computational resources. Sieving and Enumeration: algorithms that are designed to give exact solutions to the SVP problem. Security # The foundational belief of security in new lattice-based primitives stems from one key finding. Ajtai’s theorem connected the hardness of certain average-case problems to the difficulty of worst-case problems in lattices. Specifically, Ajtai demonstrated that for the Short Integer Solution (SIS) problem which we will define later, the average-case instances are at least as hard as the worst-case instances of the Shortest Vector Problem (SVP) on lattices. This means that if one could efficiently solve random instances of SIS, then they could also solve the worst-case SVP, a problem believed to be intractable even for quantum computers. Ajtai’s theorem provides a strong security guarantee for lattice-based cryptographic schemes by grounding their security in the hardness of well-studied lattice problems like SVP.\n"},{"id":1,"href":"/docs/lattice-reduction/","title":"Lattice reduction","section":"Docs","content":" Lattice reduction # Lattice reduction algorithms such as LLL and BKZ make iterative local improvements to a basis. This means that the global cost can be seen as two-folds: how costly is it to make the local improvements, which corresponds to solving an exact SVP problem and how costly is the global behavior of the algorithm. This section focuses on the global behavior of lattice reduction algorithms, while the next section (Cost models) will focus about solving local improvements. Lattice reduction is the essential tool that allows to transform an inappropriate basis to solve the SIS problem into an appropriate one and thus its complexity is the bulk of the security estimate.\nUseful quantities in lattices # Volume # In each lattice, we can define the volume of the lattice as the volume of its fundamental parallellepiped (the area delimited by the basis vectors). This quantity is an invariant of the lattice and does not depend on the basis chosen. This means that by applying Gram-Schmidt orthogonalization to any basis will give us an orthogonal basis from which we can approximate the volume of the lattice as\n$$ Vol(\\Lambda) = \\prod_{i=0}^{d-1} \\lVert \\bold{b_i}^*\\rVert $$where $\\bold{b_i}*$ are the orthogonal Gram-Schmidt vectors. It is also important to remember that $Vol(\\Lambda) = |Det(\\bold{B})|$, where $\\bold{B}$ is the basis matrix.\nThis invariant is conceptually important because it tells us that not all basis vectors can be small at the same time.\nGaussian heuristic # The gaussian heuristics predicts the number of lattice points inside any measurable body $\\mathcal{B} \\subset \\mathbb{R}^d$ is approximately $\\frac{Vol(\\mathcal{B})}{Vol(\\Lambda)}$. Applied to an euclidean d-ball, this would give that the length of the first vector is approximately\n$$\\lambda_1(\\Lambda) \\approx (\\frac{Vol(\\mathcal{B})}{Vol(\\Lambda)})^{\\frac{1}{d}} \\approx \\sqrt{\\frac{d}{2\\pi e}} Vol(\\Lambda)^{\\frac{1}{d}}$$ Root Hermite Factor # We will next want to introduce a value called the root hermite factor. It is a measure used in lattice reduction theory to evaluate the quality of a reduced lattice basis. It is commonly used to assess the effectiveness of lattice reduction algorithms. It quantifies how much longer the shortest vector in a reduced lattice basis is, compared to the length of an ideal shortest vector, scaled by the lattice dimension. Formally we define it as\n$$\\lVert \\bold{b_1} \\rVert \\approx \\delta^d Vol(\\Lambda)^\\frac{1}{d}$$ for an d-dimensional lattice.\nThe closer $\\delta$ gets to 1, the better the reduction quality will be. This is of direct impact in our context, since we need to balance a trade-off between the quality of the output basis and the cost of running our lattice reduction algorithm. In fact, with the BKZ algorithm which has become the standard, a bigger block-size leads to a better quality of output basis but also a greater computational cost.\nGeometric Series Assumption # The geometric series assumption conceptually tells us that the Gram-Schmidt vectors log-length outputed by a lattice reducion algorithm will follow a line (see the graphs in the LLL subsection). We can formulate it as:\n$$\\lVert \\bold{b_i}^*\\rVert \\approx \\alpha^{i-1}\\lVert \\bold{b_1}\\rVert$$and in fact by combining it with the root hermite factor definition we can get a relation between the quality of the reduction and the slope of the GSA assumption as $\\alpha \\approx \\delta^{-2}$ leading to\n$$ \\lVert \\bold{b_i}^*\\rVert \\approx \\alpha^{i-1} \\delta^d Vol(\\Lambda)^\\frac{1}{d} = \\delta^{-1(i+1) + d} Vol(\\Lambda)^\\frac{1}{d}$$ LLL algorithm # The Lenstra-Lenstra-Lovász (LLL) algorithm is an efficient polynomial-time algorithm in lattice theory that finds a \u0026ldquo;nearly orthogonal\u0026rdquo; basis for a given lattice. It aims to transform any arbitrary basis of a lattice into a reduced basis where the basis vectors are short and close to orthogonal following two conditions:\nSize reduction: $$1 \\leq j \u003c i \\leq d\\colon \\left|\\mu_{i,j}\\right|\\leq 0.5 \\text{ for } \\mu_{i,j} =\\frac{\\langle\\mathbf{b}_i,\\mathbf{b}^*_j\\rangle}{\\langle\\mathbf{b}^*_j,\\mathbf{b}^*_j\\rangle}$$ Lovász condition: For $k=2,\u0026hellip;,d$ $$\\omega \\Vert \\mathbf{b}^*_{k-1}\\Vert^2 \\leq \\Vert \\mathbf{b}^*_k\\Vert^2+ \\mu_{k,k-1}^2\\Vert\\mathbf{b}^*_{k-1}\\Vert^2$$ We say the basis is LLL-reduced if there exists a parameter $\\omega \\in (0.25, 1)$.\nWhile there exists some theorems bounding the worst cases of lattice reduction algorithms, they tend to perform better in practice. Reasoning about the behaviors of such algorithms has therefore become a game of heuristics and approximations. Typically, the vectors that are outputed by the LLL algorithm are said to follow the geometric series assumption in their length. Again, this assumption tells us that the shape after lattice reduction is a line with a flatter slope as lattice reduction gets stronger. The goal of lattice reduction algorithm can therefore be interpreted seen by watching a graph of the log-length of vectors after reductions. The overall goal being to flatten the line, leading to a small basis.\nCost of LLL # The theoretic bound on the quality of the LLL is $\\delta^d = (\\frac{4}{3})^\\frac{d-1}{4}$ leading to approximately $\\delta \\approx 1.075$. In practice, we get much better results on average, empirically about $\\delta \\approx 1.021$.\nIn terms of runtime, we will consider a heuristic bound (which better approximates empirical results) of $O(d^3 \\log^2(B))$.\nBKZ algorithm # The Block Korkine-Zolotarev (BKZ) algorithm is a lattice reduction algorithm that generalizes the LLL algorithm to achieve stronger reduction properties. The BKZ algorithm is defined as a blockwise reduction algorithm that iteratively applies a form of lattice basis reduction to overlapping blocks of vectors within the basis. Assuming we have an SVP oracle, the BKZ algorithm is defined as follows:\nData: LLL-reduced basis B (pre-processed) and block size beta repeat until no changes for k in 0 to d-1 LLL on local projected block [k, ..., k+beta-1] v \u0026lt;-- SVP-Oracle(local projected block[k, ..., k+beta-1]) insert v into B end A BKZ-$\\beta$ reduced basis satisfies, for $\\epsilon \u0026gt; 0$:\n$$\\lVert \\bold{b_0} \\rVert \\leq \\sqrt{(1 + \\epsilon) \\gamma_{\\beta}}^\\frac{d-1}{\\beta - 1} Vol(\\Lambda(\\bold{B}))$$, where $$\\gamma_\\beta = \\sup \\{ \\lambda_1(\\Lambda) | \\Lambda \\in \\mathbb{R}^\\beta, Vol(\\Lambda) = 1 \\}$$is the hermite constant.\nBy combining the gaussian heuristic and the definition of a BKZ-$\\beta$ reduced basis, we arrive again at the geometric assumption, which states that the length of reduced vectors follow a geometric series (which we can plot as a line as we did for LLL). This time however, it depends on the block-size chosen to run BKZ.\nWe can write\n$$\\log(\\lVert \\bold{b_i}^*\\rVert) = \\frac{d - 1 - 2i}{2}\\log(\\alpha_\\beta) + \\frac{1}{d}\\log(Vol(\\Lambda))$$where $\\alpha_\\beta$ is the slope under the geometric assumption that can be calculated from the gaussian assumption as\n$$\\alpha_\\beta = \\sqrt{\\frac{d}{2\\pi e}}^\\frac{2}{\\beta - 1}$$This estimate is reasonably accurate only if $d\\gg\\beta$ and $\\beta \u0026gt; 50$, which is why we will use fixed estimates for small dimensions.\nCost of BKZ # Costing BKZ means having a good idea of the impact of the block-size on the quality of our reduced basis. For this, we could either make the approximation $\\delta_\\beta \\approx \\sqrt \\alpha_\\beta$ or use the following limit defined in $$\\lim_{\\beta\\rightarrow\\infty}\\delta_\\beta = (\\frac{\\beta}{2\\pi e}(\\pi\\beta)^\\frac{1}{\\beta})^\\frac{1}{2(\\beta - 1)}$$ and write for SIS\n$$\\lVert \\bold{b_1} \\rVert \\approx \\delta_\\beta^{d-1} Vol(\\Lambda)^{\\frac{1}{d}}$$ Costing BKZ as a whole is complicated because we do not know how many tours we will have to run, which means we don\u0026rsquo;t really know in advance the number of SVP-Oracle calls we will have to make. Furthermore, many improvements on plain BKZ have been made when some techniques are used as a subroutine for the oracle (for example extreme pruning in the context of enumeration), which makes security estimates done via lattice reduction very sensitive to many factors. Also, local preprocessing techniques in a variant of BKZ known as progressive BKZ. To make our tool comparable to the lattice estimator by \u0026hellip;[insert], we will follow the same simplifying assumption and consider a consistent 8 tours of BKZ. This makes sense following experimental results that showed that most progress is made in the 7-9 first tours. We will then use: $$cost = \\tau \\cdot d \\cdot T_{SVP}$$ where the number of tours we do $\\tau$ is considered to be 8. The number of times the SVP oracle is called per tour, which is dimension of the lattice, is d The cost of the SVP oracle is $T_{SVP}$ Further improvements # It has been showed in practice that the GSA assumption is a lie. In reality it behaves differently at its tail and this behaviour can be simulated.\nThe dimension for free\nAlso, a lot of tweaks, additions and speed up have been introduced into plain BKZ leading to BKZ 2.0, which provides a stronger reduction. See CN11 Simulator.\nBehaviour in q-ary lattices (ZGSA)\nMatzov Improvement\n"},{"id":2,"href":"/docs/cost-models/","title":"Cost Models","section":"Docs","content":" Cost Models # Given the simplified cost for BKZ behaviour that we consider $cost = \\tau \\cdot d \\cdot T_{SVP}$, we still need to define the cost of the SVP solver. Sieving and enumeration are the two common strategies to find the shortest non-zero vector in a lattice.\nSieving # Sieving algorithms for SVP work by iteratively refining a large set of lattice vectors to obtain progressively shorter vectors until the shortest one is found. In its basic form, the sieving process starts by generating a large set of random lattice vectors, often called a \u0026ldquo;cloud.\u0026rdquo; Pairs of vectors are then combined (usually by adding or subtracting them) to produce shorter vectors, which are then added back to the cloud if they meet certain criteria. This process continues until the vectors in the cloud converge towards the shortest lattice vector. Modern sieving methods, like the GaussSieve or ListSieve, have been optimized to handle higher-dimensional lattices by limiting pairwise vector interactions, which reduces computational complexity. Sieving requires storing a large number of lattice vectors and, therefore, can be memory-intensive, especially as the lattice dimension grows.\nIn terms of cost, sieving algorithm can solve the SVP in a lattice of dimension d in $2^{O(k)}$ time but at the cost og a much higher memory usage. At the risk of being overly conservative, our estimator will disregard memory usage.\nThe following sieving estimates are available:\nName Reference Cost Regime BDGL-sieve todo $2^{0.292\\beta + 16.4}$ big $\\beta$ $2^{0.387\\beta + 16.4}$ small $\\beta$ classical Q-sieve todo $2^{0.265\\beta}$ quantum ADPS-sieve tedo $2^{0.292\\beta}$ classical BJG-sieve todo $2^{0.311\\beta}$ classical ChaLoy-sieve todo $2^{0.257\\beta}$ quantum Here are some plots to better visualize sieving costs.\nEnumeration # Enumeration algorithms systematically search through lattice points in a controlled way, typically by traversing lattice vectors within a fixed radius from the origin. They rely on a recursive process to explore potential candidate vectors within a \u0026ldquo;search region,\u0026rdquo; using techniques to prune paths that are unlikely to lead to the shortest vector. Enumeration is typically carried out with the help of a basis that has been reduced (made close to orthogonal), as this greatly improves efficiency by minimizing the number of candidate paths. Unlike sieving, enumeration methods are deterministic and guarantee finding the shortest vector by systematically exploring all feasible paths. The efficiency of enumeration depends strongly on the quality of the lattice basis. Preprocessing steps like BKZ (Block Korkine-Zolotarev) reduction can make enumeration significantly faster by transforming the basis to be more suitable for search. Enumeration is often practical for lower-dimensional lattices or when a high degree of accuracy is needed, but it tends to be less efficient than sieving in high dimensions due to its exponential complexity.\nIn terms of cost, enumeration can solve SVP in a lattice of dimension d in $2^{O(k \\log k)}$ time and space.\nThe following enumeration estimates are available:\nName Reference Cost Regime Lotus todo $2^{0.125\\beta\\log\\beta -0.755\\beta + 2.254}$ classical ABFSK-enum todo $2^{0.184\\beta\\log\\beta - 0.995\\beta + 22.25}$ small $\\beta$ $2^{0.125\\beta\\log\\beta-0.547\\beta+16.4}$ big $\\beta$ classical ABFSK-Q-enum todo $2^{0.311\\beta}$ quantum ABLR-enum todo $2^{0.184\\beta\\log\\beta - 1.077\\beta + 35.12}$ small $\\beta$ $2^{0.125\\beta\\log\\beta-0.655\\beta+31.84}$ big $\\beta$ classical Here are some plots to better visualize sieving costs.\n"},{"id":3,"href":"/docs/sis/","title":"SIS","section":"Docs","content":" Short Integer Solution problem (SIS) # Let us look at two different ways to define q-ary lattices. From \\(\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}\\)\n$$\\Lambda_q(\\bold{A^T}) = \\{\\bold{y} \\in \\mathbb{Z}^w : \\bold{y} = \\bold{A}^T\\bold{s} \\text{ mod q }\\text{ for some } \\bold{s} \\in \\mathbb{Z}^h\\}$$$$\\Lambda^T_q(\\bold{A}) = \\{\\bold{y} \\in \\mathbb{Z}^w : \\bold{A}\\bold{y} = \\bold{0} \\text{ mod q }\\}$$The first lattice is generally called the primal problem and relates to the LWE problem since finding a short vector in \\(\\Lambda_q(\\bold{A^T})\\) corresponds to solving the LWE problem. The second lattice is generally called the dual problem and relates to the SIS problem since finding a short vector in \\(\\Lambda^T_q(\\bold{A})\\) corresponds to solving the SIS problem.\nFormal definition # We define \\(SIS(h, q, w, \\beta)\\) as follows:\nGiven \\(\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}\\) find the short vector \\(\\bold{s} \\in \\mathbb{Z}^w\\) where \\(0 \u0026lt; \\lVert s \\rVert_p \\leq \\beta\\).\nWe can note that the problem becomes trivial as soon as \\(\\beta \\geq q\\), no matter the norm used.\nEstimating SIS hardness # Estimating the hardness of an SIS instance is done via estimating the number of operations required to run lattice reductions attacks on the related SVP problem. This concretely means that we need to know:\nHow small we expect the shortest vector to be in our SVP problem. How small we expect vectors to be after lattice reduction. How to cost lattice reduction (see the lattice reduction section). We will mostly focus on BKZ 2.0 as a reduction algorithm and we will present how we estimate all parts of the attack.\nL2-Norm strategy # "},{"id":4,"href":"/docs/sis/module-sis/","title":"Module Sis","section":"SIS","content":" Module-SIS # "},{"id":5,"href":"/docs/sis/ring-sis/","title":"Ring Sis","section":"SIS","content":" Ring-SIS # "}]