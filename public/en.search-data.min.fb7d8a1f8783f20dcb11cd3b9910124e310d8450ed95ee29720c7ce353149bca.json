[{"id":0,"href":"/lattirust_estimator.io/docs/preliminaries/","title":"Preliminaries","section":"Docs","content":" Preliminaries # This section is based on the following resources [1][2][3][4].\nLattices # A lattice is defined as a discrete subgroup in n-dimensional $\\mathbb{R}^n$ space with a periodic structure. It can be represented by a set of linearly independent vectors commonly named the basis of the lattice. If $\\bold{b_1}, \u0026hellip;, \\bold{b_d}$ denote basis vectors we can then describe a lattice by:\n$$\\Lambda(\\bold{b_1}, ..., \\bold{b_d}) = \\{\\sum_{i=1}^{d}x_i\\bold{b_i}: x_i \\in \\mathbb{Z}\\}$$ which is the set of all linear combinations of basis vectors. $\\it{d}$ is the dimension of the lattice in $\\mathbb{R}^n$.\nA lattice in 2D A lattice has many bases but some are more interesting than others. The goal of lattice reduction presented in the future sections is to find a qualitatively good basis composed of short and almost orthogonal vectors.\nIn the rest of this work, we will especially consider q-ary lattices described by a basis $\\bold{B}$, in which coefficients are taken modulo q. The membership of a vector to the lattice is then determined by $\\bold{x} \\text{ mod q}$.\nVolume of a lattice # In each lattice, we can define the volume of the lattice as the volume of its fundamental parallellepiped (the area delimited by the basis vectors). This quantity is an invariant of the lattice and does not depend on the basis chosen. This means applying Gram-Schmidt orthogonalization to any basis will give us an orthogonal basis from which we can approximate the volume of the lattice as\n$$ Vol(\\Lambda) = \\prod_{i=0}^{d-1} \\lVert \\bold{b_i}^*\\rVert $$where $\\bold{b_i}*$ are the orthogonal Gram-Schmidt vectors. It is also good to remember that $Vol(\\Lambda) = |Det(\\bold{B})|$, where $\\bold{B}$ is the basis matrix.\nA lattice volume in 2D This invariant is conceptually important because it tells us that not all basis vectors can be small at the same time.\nDual of a lattice # The dual of a lattice $\\Lambda$ in $\\mathbb{R}^d$ is denoted $\\Lambda^*$ and is the lattice defined by vectors $\\bold{y}$ such that:\n$$\\langle \\bold{x}, \\bold{y}\\rangle \\in \\mathbb{Z}, \\forall \\bold{x} \\in \\Lambda$$We have that for any lattice described by basis $\\bold{B}$,\n$$\\Lambda(\\bold{B})^*=\\Lambda((\\bold{B}^{-1})^T) \\text{ and } Vol(\\bold{\\Lambda^*})=\\frac{1}{Vol(\\bold{\\Lambda})}$$ Minima of a lattice and root hermite factor # We denote by $\\lambda_i(\\bold{B})$ the $i$-th minimum of a lattice described by a basis B, note that for the rest of the work we sometimes interchangeably use $\\lambda_i(\\bold{B})$, $\\lambda_i(\\Lambda(\\bold{B}))$ or $\\lambda_i(\\Lambda)$ for minimas and other measures. We can think of it as the radius of the smallest zero-centered ball containing at least $i$ linearly independent lattice vectors. The root hermite constant $\\gamma_n$ is a constant that determines how short a lattice element can be. It is known exactly only for $n\\in \\lbrace 1, 2, 3, 4, 5, 6, 7, 8, 24 \\rbrace$ but we know upper bounds for other values and can be found in [5]. In general, determining $\\lambda_i(\\bold{B})$ is hard so we usually refer to upper bounds.\nMinkowksi second theorem: For any lattice in $\\mathbb{R}^n$, and any $1 \\leq d \\leq n$, we have\n$$ (\\prod^{d}_{i=1} \\lambda_i{(\\Lambda)})^{\\frac{1}{d}} \\leq \\sqrt{\\gamma_n}Vol(\\Lambda)^{\\frac{1}{n}}$$Formally, the hermite constant over a full rank lattice in $\\mathbb{R}^n$ is defined as:\n$$\\gamma_n = \\sup_{\\Lambda \\subset \\mathbb{R}^n} \\frac{\\lambda_1^2(\\Lambda)}{\\det(\\Lambda)^{2/n}}$$and it is closely related to the root hermite factor $\\delta_n = \\left( \\frac{\\lambda_1^2}{\\det(\\Lambda)^{2/n}} \\right)^{1/2}$ that we will present in more detail in the lattice reduction chapter. It is used to qualify the quality of a basis and we have $\\delta_n = \\sqrt{\\gamma_n}$.\nRandom lattices and Gaussian heuristic # The notion of a random lattice is somewhat mathematically complicated (see Haar measures), as a simplifying assumption, imagine such a lattice to be sampled from an existing random distribution to follow with overwhelming probability the gaussian heuristic that follows.\n$$\\frac{\\lambda_i(\\Lambda)}{\\text{Vol}(\\Lambda)^{1/n}} \\approx \\frac{\\Gamma\\left(1 + \\frac{n}{2}\\right)^{1/n}}{\\sqrt{\\pi}} \\approx \\sqrt{\\frac{n}{2\\pi e}}$$The gaussian heuristic predicts that the number of lattice points inside any measurable body $\\mathcal{B} \\subset \\mathbb{R}^d$ is approximately $\\frac{Vol(\\mathcal{B})}{Vol(\\Lambda)}$. Applied to an euclidean d-ball, this would give that the length of the first vector is approximately:\n$$\\lambda_1(\\Lambda) \\approx (\\frac{Vol(\\mathcal{B})}{Vol(\\Lambda)})^{\\frac{1}{d}} \\approx \\sqrt{\\frac{d}{2\\pi e}} Vol(\\Lambda)^{\\frac{1}{d}}$$ Hard Problems in Lattices # The security of lattice-based constructions relies on several variants of the same problem. The foundation of security is based on the fact that finding a short non-zero vector in a lattice is computationally hard. Below are definitions of the key problems:\nShortest Vector Problem (SVP)\nDefinition:\nGiven a lattice $\\Lambda$ with basis $\\bold{B}$ , the Shortest Vector Problem (SVP) is the problem of finding a non-zero vector $\\bold{v} \\in \\Lambda$ such that:\n$$\\|\\bold{v}\\| = \\lambda_1(\\Lambda)$$where $\\lambda_1(\\Lambda)$ is the length of the shortest non-zero vector in the lattice. This is a well-known NP-hard problem in the worst case, and its approximation remains challenging for cryptographic settings.\nHermite Shortest Vector Problem (H-SVP)\nDefinition:\nThe Hermite Shortest Vector Problem (H-SVP) is a scaled version of the SVP. It asks for a non-zero vector $\\bold{v} \\in \\Lambda$ such that:\n$$\\|\\bold{v}\\| \\leq \\delta_n \\cdot \\text{Vol}(\\Lambda)^{1/n}$$where $\\delta_n$ is the root Hermite factor. Unlike exact SVP, H-SVP allows finding a vector that is \u0026ldquo;short enough\u0026rdquo; relative to the lattice volume. H-SVP is easier to approximate than SVP and is central to lattice reduction algorithms like LLL or BKZ.\nApproximate Shortest Vector Problem (Approx-SVP)\nDefinition:\nThe Approximate Shortest Vector Problem (Approx-SVP) generalizes SVP by relaxing the requirement to find the exact shortest vector. Given a lattice $\\Lambda$ and an approximation factor $\\gamma \\geq 1$, the goal is to find a non-zero vector $\\bold{v} \\in \\Lambda$ such that:\n$$\\|\\bold{v}\\| \\leq \\gamma \\cdot \\lambda_1(\\Lambda)$$For sufficiently large $\\gamma$, Approx-SVP becomes computationally easier, but it remains hard for small $\\gamma$, especially in high-dimensional lattices.\nUnique Shortest Vector Problem (Unique-SVP)\nDefinition:\nThe Unique Shortest Vector Problem (Unique-SVP) is a variant of SVP where the shortest vector is guaranteed to be significantly shorter than all other lattice vectors. Specifically, there exists a gap $\\beta \u0026gt; 1$ such that:\n$$\\lambda_2(\\Lambda) \\geq \\beta \\cdot \\lambda_1(\\Lambda)$$where $\\lambda_2(\\Lambda)$ is the length of the second shortest lattice vector. The task is to find the unique shortest vector $\\bold{v} \\in \\Lambda$. This problem is somewhat easier than general SVP due to the presence of a unique solution, but it still remains computationally challenging.\nClosest Vector Problem (CVP)\nDefinition:\nGiven a lattice $\\Lambda$ with basis $\\bold{B}$ and a target vector$\\bold{t} \\in \\mathbb{R}^n$, the Closest Vector Problem (CVP) is the problem of finding a vector $\\bold{v} \\in \\Lambda$\n$$\\|\\bold{t} - \\bold{v}\\| = \\min_{\\bold{w} \\in \\Lambda} \\|\\bold{t} - \\bold{w}\\|$$CVP is generally harder than SVP and is also NP-hard in the worst case. Approximation versions of CVP, like $\\gamma$-CVP (finding $\\bold{v}$ within a $\\gamma$-factor of the closest vector), are frequently studied in lattice cryptography.\nShortest Independent Vector Problem (SIVP)\nDefinition:\nThe Shortest Independent Vector Problem (SIVP) involves finding \\( n \\) linearly independent vectors \\( \\{\\bold{v}_1, \\bold{v}_2, \\dots, \\bold{v}_n\\} \\) in a lattice \\( \\Lambda \\) such that the maximum norm of the vectors is minimized:\n$$ \\max_{i=1}^n \\|\\bold{v}_i\\| = \\lambda_n(\\Lambda) $$where \\( \\lambda_n(\\Lambda) \\) represents the length of the \\( n \\)-th successive minimum of the lattice. SIVP is closely related to the geometry of the lattice and remains computationally hard. It is used in cryptographic constructions and reduction proofs, often connecting the hardness of lattice problems to cryptographic assumptions.\nSummary Table of Lattice Problems # Problem Goal Output Difficulty (I)SVP Find the shortest vector \\( \\bold{v} \\in \\Lambda, \\|\\bold{v}\\| = \\lambda_1 \\) NP-hard in the worst case H-SVP Find a short enough vector \\( \\bold{v} \\in \\Lambda, \\|\\bold{v}\\| \\leq \\delta_n \\cdot \\text{Vol}(\\Lambda)^{1/n} \\) Easier than SVP, used in reduction Approx-(I)SVP Approximate the shortest vector \\( \\bold{v} \\in \\Lambda, \\|\\bold{v}\\| \\leq \\gamma \\lambda_1 \\) Hard for small \\( \\gamma \\) Unique-(I)SVP Find the unique shortest vector \\( \\bold{v} \\in \\Lambda, \\lambda_2 \\geq \\beta \\lambda_1 \\) Easier than SVP (gap \\( \\beta \\)) CVP Find the closest lattice vector to target \\( \\bold{v} \\in \\Lambda, \\|\\bold{t} - \\bold{v}\\| \\) NP-hard, harder than SVP Importantly for later, any algorithm solving Approx-SVP with factor $\\alpha$ also solves Hermite-SVP for factor $\\alpha\\sqrt{\\gamma_n}$ in polynomial time. Also any algoritm solving H-SVP in $\\alpha$ can be used a linear number of times to solves Approx-SVP with factor $\\alpha^2$ in polyomial time.[6]There also exists from worst-case Approx-SVP to average-case H-SVP for certain class of lattices.\nSecurity # The foundational belief of security in new lattice-based primitives stems from one key finding. Ajtai’s theorem connected the hardness of certain average-case problems to the difficulty of worst-case problems in lattices [7]. Specifically, Ajtai demonstrated that for the Short Integer Solution (SIS) problem which we will define later, the average-case instances are at least as hard as the worst-case instances of the Shortest Vector Problem (SVP) on lattices. This means that if one could efficiently solve random instances of SIS, then they could also solve the worst-case SVP, a problem believed to be intractable even for quantum computers. Ajtai’s theorem provides a strong security guarantee for lattice-based cryptographic schemes by grounding their security in the hardness of well-studied lattice problems like SVP.\nThe security beliefs (hardness of SVP) can be summarized by two conjectures from [3]:\nThere is no polynomial time algorithm that approximates lattice problems to within polynomial factors. There is no polynomial time quantum algorithm that approximates lattice problems within polynomial factors. Norms # We give a brief overview of norms we will use later on and their relationships. We will also need norms in modulus and rings.\nDefinition of \\( \\ell_p \\)-Norms # For a vector $\\boldsymbol{f} \\in \\mathbb{R}^n$, the $\\ell_p$-norms are defined as follows:\n$$ \\|\\boldsymbol{f}\\|_1 = \\sum_{i=1}^n |f_i|, \\quad \\|\\boldsymbol{f}\\|_2 = \\sqrt{\\sum_{i=1}^n f_i^2}, \\quad \\|\\boldsymbol{f}\\|_p = \\left(\\sum_{i=1}^n |f_i|^p\\right)^{1/p}, \\quad \\|\\boldsymbol{f}\\|_\\infty = \\max_{i} |f_i|. $$These norms generalize to module elements $\\boldsymbol{f} \\in R_q^m$ (where $R_q$ is a quotient ring, e.g., $\\mathbb{Z}_q[x]/\\langle x^n + 1 \\rangle$), by viewing them as $m \\cdot n$-dimensional vectors[8]. Also remember that module generalize rings, so a vector $\\bold{f}$ in the ring $R_q^1$ is a set of coefficients $f_i$ such that $\\bold{f}=\\sum_if_ix^i$\nRelationships Between Norms # For $p, q \\geq 1$ with $p \\leq q$, the following relationships hold for $\\boldsymbol{f} \\in \\mathbb{R}^n$:\n$$ \\|\\boldsymbol{f}\\|_\\infty \\leq \\|\\boldsymbol{f}\\|_q \\leq \\|\\boldsymbol{f}\\|_p \\leq \\|\\boldsymbol{f}\\|_1. $$This means that the $\\ell_\\infty$-norm is always the smallest, while the $\\ell_1$-norm is the largest.\nApplications in lattice problem hardness estimation # Norms are crucial for bounding errors and ensuring security in lattice-based cryptography. For example:\nIn the Learning with Errors (LWE) problem, the $\\ell_2$-norm is used to bound the error vector. In the Short Integer Solution (SIS) problem, the $\\ell_\\infty$-norm is often employed to restrict the coefficients of the solution. By carefully analyzing and relating these norms, cryptographic schemes ensure robustness against attacks while maintaining efficiency.\nReferences # N. Gama and\u0026#32;P. Nguyen,\u0026#32;Predicting lattice reduction,\u0026#32;In Proc. Advances in cryptology–EUROCRYPT 2008: 27th annual international conference on the theory and applications of cryptographic techniques, istanbul, turkey, april 13-17, 2008. Proceedings 27,\u0026#32;2008, pp. 31–51.\u0026#32;D. Micciancio and\u0026#32;S. Goldwasser,\u0026#32;Complexity of lattice problems: A cryptographic perspective.\u0026#32;Springer Science \u0026amp; Business Media, 2002.\u0026#32;D. Micciancio and\u0026#32;O. Regev,\u0026#32;Lattice-based cryptography,\u0026#32;in Post-quantum cryptography,\u0026#32;Springer, 2009, pp. 147–191.P. Nguyen,\u0026#32;Hermite’s constant and lattice algorithms,\u0026#32;in The LLL algorithm: Survey and applications,\u0026#32;Springer, 2009, pp. 19–69.H. Cohn and\u0026#32;N. Elkies,\u0026#32;New upper bounds on sphere packings i,\u0026#32;Annals of mathematics,\u0026#32;pp. 689–714,\u0026#32;2003.\u0026#32;L. Lovász,\u0026#32;An algorithmic theory of numbers, graphs and convexity.\u0026#32;SIAM, 1986.\u0026#32;M. Ajtai,\u0026#32;Generating hard instances of lattice problems,\u0026#32;In Proc. Proceedings of the twenty-eighth annual ACM symposium on theory of computing,\u0026#32;1996, pp. 99–108.\u0026#32;C. Baum,\u0026#32;I. Damgård,\u0026#32;V. Lyubashevsky,\u0026#32;S. Oechsner, and\u0026#32;C. Peikert,\u0026#32;More efficient commitments from structured lattice assumptions,\u0026#32;In Proc. International conference on security and cryptography for networks,\u0026#32;2018, pp. 368–385.\u0026#32; "},{"id":1,"href":"/lattirust_estimator.io/docs/lattice-reduction/","title":"Lattice reduction","section":"Docs","content":" How to solve SVP ? # The way we solve the Shortest Vector Problem (SVP) and similar problems depends a lot on the lattice dimension. In lower dimensions, exact solvers are practical, and there are two main approaches: enumeration and sieving. Both methods perform some exhaustive search over all short lattice vectors—enumeration does this deterministically, while sieving is typically randomized. However, as the lattice dimension grows, the number of possible solutions increases exponentially, making these methods infeasible for high dimensions (think 100 and more).\nIn higher dimensions, we rely on approximation algorithms, better known as lattice reduction algorithms. These algorithms don’t find the exact solution but instead provide an approximation where the vector length is upper bounded by a function of the dimension. Lattice reduction can be thought of as the algorithmic equivalent of inequalities like Hermite’s and Mordell’s [1], which bound the shortest vector length in theoretical terms.\nHermite\u0026rsquo;s inequality: $\\forall d \\geq 2: \\gamma_d \\leq (\\sqrt{\\frac{4}{3}})^{d-1}$\nMordell\u0026rsquo;s inequality: $\\forall d,k \\text{ such that } 2\\leq k \\leq d: \\gamma_d \\leq \\sqrt{\\gamma_k}^{\\frac{d-1}{k-1}}$\nIn practice, both exact and approximate solvers are used together. Exact solvers usually start with a preprocessing step using lattice reduction to simplify the problem. On the other hand, lattice reduction algorithms often call exact solvers as subroutines, using them multiple times during their process. This section focuses on explaining the cost of approximation algorithm such as LLL and BKZ as a whole and we will refer to the cost of the exact SVP solver used as a subroutine as the \u0026ldquo;cost models\u0026rdquo;, described in the next section.\nLattice reduction # Lattice reduction algorithms aim to transform a given basis of a lattice into a \u0026ldquo;reduced\u0026rdquo; basis, where the vectors are shorter and closer to being orthogonal. Indeed, while a lattice may not have an orthogonal basis in contrary from Euclidean space, the goal of lattice reduction is to get closer to a basis that is not far from orthogonal. This section is mostly based on the following works [1][2][3][4][5][6][7].\nTwo different bases, one being close to orthogonal. This reduction makes it easier to approximate solutions to hard lattice problems like the Shortest Vector Problem. In short, the quality of a basis can be improved to make the problem easier. The most prominent lattice reduction algorithms include:\nLLL Algorithm (Lenstra–Lenstra–Lovász): it produces a reduced basis in polynomial time, where the vectors are guaranteed to be within a known factor of the shortest vector, but doesn’t necessarily find the shortest vector [5]. BKZ Algorithm (Block Korkine-Zolotarev): the BKZ algorithm is a generalization of the LLL algorithm and provides better reduction at the cost of higher computational complexity. By working in blocks of the lattice and applying LLL reduction to these blocks, BKZ achieves stronger approximations of the shortest vector, though it requires more computational resources [4]. As explained previously, LLL and BKZ make iterative local improvements to a basis by calling an exact SVP oracle. This means that the global cost can be seen as two-folds: how costly is it to make the local improvements, and how costly is the global behavior of the algorithm.\nGram-Shmidt orthogonalization # Gram-Schmidt orthogonalization is a method for orthonormalizing a set of vectors in an inner product space, most commonly the Euclidean space $\\mathbb{R}^n$. The process transforms a set of linearly independent vectors into an orthonormal set of vectors that spans the same subspace.\nGiven a set of linearly independent vectors $\\lbrace\\mathbf{b}_1, \\mathbf{b}_2, \\ldots, \\mathbf{b}_n\\rbrace$, the Gram-Schmidt process produces an orthogonal set $\\lbrace\\mathbf{b}^*_1, \\ldots, \\mathbf{b}^*_n\\rbrace$ as follows:\n$\\mathbf{b}_1^* = \\mathbf{b}_1$ For $i = 2$ to $n$: $$ \\mathbf{b}_i^* = \\mathbf{b}_i - \\sum_{j=1}^{i-1} \\text{proj}_{\\mathbf{b}_j^*}(\\mathbf{b}_i) $$ where $\\text{proj}_{\\mathbf{b}_j^*}(\\mathbf{b}_i)$ is the projection of $\\mathbf{b}_i$ onto $\\mathbf{b}_j^*$, given by: $$ \\text{proj}_{\\mathbf{b}_j^*}(\\mathbf{b}_i) = \\frac{\\langle \\mathbf{b}_i, \\mathbf{b}_j^* \\rangle}{\\langle \\mathbf{b}_j^*, \\mathbf{b}_j^* \\rangle} \\mathbf{b}_j^* = \\mu_{i,j} $$ Gram-Schmidt orthogonalization is widely used in lattice reduction is because it allows to triangularize the basis. More precisely, we can get a new basis:\n$$ \\begin{pmatrix} \\|\\mathbf{b}_1^*\\| \u0026 0 \u0026 \\ldots \u0026 0 \\\\ \\mu_{2,1} \\|\\mathbf{b}_1^*\\| \u0026 \\|\\mathbf{b}_2^*\\| \u0026 \\ldots \u0026 0 \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\mu_{d,1} \\|\\mathbf{b}_1^*\\| \u0026 \\mu_{d,2} \\|\\mathbf{b}_2^*\\| \u0026 \\ldots \u0026 \\|\\mathbf{b}_d^*\\| \\end{pmatrix} $$So $B = \\mu B^*$ [7][1]. And we can easily confirm from the matrix that $Vol(\\Lambda) =\\prod_{i=0}^{d-1} \\lVert \\bold{b_i}^*\\rVert$ (the determinant is the diagonal). We can also state the following lemma that relates our shortest vector to the Gram-Schmidt vectors for all $1 \\leq i \\leq d$:\n$$ \\lambda_i(\\Lambda) \\geq \\min_{i \\leq j \\leq d} \\|\\mathbf{b}_j^*\\|. $$Remember that the volume is an invariant so not all GS vectors can be small at the same time.\nRoot hermite factor # We will next want to introduce a value called the root hermite factor. It is a measure used in lattice reduction theory to evaluate the quality of a reduced lattice basis. It is commonly used to assess the effectiveness of lattice reduction algorithms.\n$$ \\delta = \\left( \\frac{\\|\\mathbf{b}_1\\|}{\\text{vol}(\\Lambda)^{1/d}} \\right)^{1/d} $$ for a d-dimensional lattice.\nThe closer $\\delta$ gets to 1, the better the reduction quality will be. This is of direct impact in our context, since we need to balance a trade-off between the quality of the output basis and the cost of running our lattice reduction algorithm. In fact, with the BKZ algorithm which has become the standard, a bigger block-size leads to a better quality of output basis (so a better $\\delta_\\beta$) but also a greater computational cost.\nGeometric Series Assumption (GSA) # How large the minimas can be after lattice reduction is therefore looking for how short we expect the basis vector to be after applying Gram-Schmidt ortogonalization followed by lattice reduction. It is often useful to look at the length of all gram-schmidt vectors, not only the first one. As a small experiment, let us consider a basis in $\\mathbb{Z}_q$ and let us compute the Gram-Schmidt orthogonalization. If we look at the log of these lengths we obtain the Z-shape because the first are orthogonal components of q magnitude and the rest are all vectors of length 1:\nLength of log GS vectors If we now apply a lattice reduction algorithm (here LLL), we will obtain this:\nLength of vectors after LLL You can observe this merely looks like a straight and indeed this the assumption that we will make. The Geometric Series Assumption conceptually tells us that the Gram-Schmidt vectors log-length outputed by a lattice reducion algorithm will follow a geometric series and such a line in log lengths. We can formulate it as:\n$$\\lVert \\bold{b_i}^*\\rVert \\approx \\alpha^{i-1}\\lVert \\bold{b_1}\\rVert$$and in fact by combining it with the fact that output vector of lattice reduction follow $\\lVert \\bold{b_0} \\rVert = \\delta_0 Vol(\\Lambda)^{\\frac{1}{d}}$ we can get a relation between the quality of the reduction and the slope of the GSA assumption as $\\alpha \\approx \\delta^{-2}$ leading to\n$$ \\lVert \\bold{b_i}^*\\rVert \\approx \\alpha^{i-1} \\delta^d Vol(\\Lambda)^\\frac{1}{d} = \\delta^{-1(i+1) + d} Vol(\\Lambda)^\\frac{1}{d}$$ LLL algorithm # The Lenstra-Lenstra-Lovász (LLL) algorithm is an efficient polynomial-time algorithm that finds a \u0026ldquo;nearly orthogonal\u0026rdquo; basis for a given lattice. It aims to transform any arbitrary basis of a lattice into a reduced basis where the basis vectors are short and close to orthogonal following two conditions:\nSize reduction: $$1 \\leq j \u003c i \\leq d\\colon \\left|\\mu_{i,j}\\right|\\leq 0.5 \\text{ for } \\mu_{i,j} =\\frac{\\langle\\mathbf{b}_i,\\mathbf{b}^*_j\\rangle}{\\langle\\mathbf{b}^*_j,\\mathbf{b}^*_j\\rangle}$$ Lovász condition: For $k=2,\u0026hellip;,d$ $$\\omega \\Vert \\mathbf{b}^*_{k-1}\\Vert^2 \\leq \\Vert \\mathbf{b}^*_k\\Vert^2+ \\mu_{k,k-1}^2\\Vert\\mathbf{b}^*_{k-1}\\Vert^2$$ We say the basis is LLL-reduced if there exists a parameter $\\omega \\in (0.25, 1)$. Hereafter we give a pseudo code of the algorithm and a graphical example of its run.\nData: a basis B Repeat until no changes: for k = 0 to d-1: # Step 1: Gram-Schmidt Orthogonalization (GSO) for i = k+1 to d-1: mu[k, i] = \u0026lt;B[k], B[i]\u0026gt; / \u0026lt;B[k], B[k]\u0026gt; B[i] = B[i] - mu[k, i] * B[k] # Step 2: Size Reduction for i = k+1 to d-1: if |mu[k, i]| \u0026gt;= 1/2: v = round(mu[k, i]) B[i] = B[i] - v * B[k] # Step 3: Swap if necessary if \u0026lt;B[k+1], B[k+1]\u0026gt; \u0026lt; (delta * \u0026lt;B[k], B[k]\u0026gt;): Swap B[k], B[k+1] end Example of the LLL algorithm running. While there exists some theorems bounding the worst cases of lattice reduction algorithms, they tend to perform better in practice. Reasoning about the behaviors of such algorithms has therefore become a game of heuristics and approximations. Typically, the vectors that are outputed by the LLL algorithm are said to follow the geometric series assumption in their length. Again, this assumption tells us that the shape after lattice reduction is a line with a flatter slope as lattice reduction gets stronger. The goal of lattice reduction algorithm can therefore be interpreted seen by watching a graph of the log-length of vectors after reductions. The overall goal being to flatten the line, leading to a small basis.\nCost of LLL # The theoretic bound on the quality of the LLL is $\\delta^d = (\\frac{4}{3})^\\frac{d-1}{4}$ [1] leading to approximately $\\delta \\approx 1.075$. In practice, we get much better results on average, empirically about $\\delta \\approx 1.021$ .\nIn terms of runtime, we will consider a heuristic bound (which better approximates empirical results) of $O(d^3 \\log^2(B))$.\nBKZ algorithm # The Block Korkine-Zolotarev (BKZ) algorithm is a lattice reduction algorithm that generalizes the LLL algorithm to achieve stronger reduction properties. The BKZ algorithm is defined as a blockwise reduction algorithm that iteratively applies a form of lattice basis reduction to overlapping blocks of vectors within the basis. The assumption made in its analysis is that iterative blocks taken behaves like a random lattice. It is in fact a relaxation of the Hermite-Korkine-Zolotarev (HKZ). The HKZ reduction is a stronger form of lattice reduction that ensures each vector in the basis is the shortest vector in the lattice projected orthogonally onto the space spanned by the preceding basis vectors.\nHKZ reduction # Let b₁, b₂, \u0026hellip;, bₙ be a basis of a lattice L.\nThe basis is said to be HKZ-reduced if:\nb₁ is the shortest vector in the lattice L. For i = 2, 3, \u0026hellip;, n, the vector bᵢ is the shortest vector in the lattice L projected orthogonally onto the span of b₁, b₂, \u0026hellip;, bᵢ₋₁. BKZ # Assuming we have an SVP oracle, the BKZ algorithm is defined as follows:\nData: LLL-reduced basis B (preprocessed) and block size beta repeat until no changes for k in 0 to d-2 LLL on local projected block B[k, ..., min(k+beta, d)] v \u0026lt;-- SVP-Oracle(local projected block B[k, ..., min(k+beta, d)]) insert v into B at index k and handle linear dependencies with LLL end In practice, stronger preprocessing than LLL is often performed before the BKZ block loop. Hereafter is a simple representation of a block moving through a matrix for one tour of BKZ, in practice we often do several tours because we are supposed to stop when no more significant changes occur.\nExample of the BKZ algorithm block sliding. Theoretically, a BKZ-$\\beta$ reduced basis satisfies, for $\\epsilon \u0026gt; 0$:\n$$\\lVert \\bold{b_0} \\rVert \\leq \\sqrt{(1 + \\epsilon) \\gamma_{\\beta}}^{(\\frac{d-1}{\\beta - 1} + 1)} Vol(\\Lambda(\\bold{B}))$$$$\\gamma_\\beta = \\sup \\{ \\lambda_1(\\Lambda) | \\Lambda \\in \\mathbb{R}^\\beta, Vol(\\Lambda) = 1 \\}$$Several variants of BKZ exist, the one giving asymptotically the best worst-case guarantees is the Slide reduction described in [8] and it achieves\n$$\\lVert \\bold{b_0} \\rVert \\leq \\sqrt{(1 + \\epsilon) \\gamma_{\\beta}}^\\frac{d-1}{\\beta - 1} Vol(\\Lambda(\\bold{B}))$$By combining the gaussian heuristic and the definition of a BKZ-$\\beta$ reduced basis, we arrive again at the geometric assumption, which states that the log-lengths of reduced vectors follow a geometric series (which we can plot as a line as we did for LLL). This time however, it depends on the block-size chosen to run BKZ.\nNote: maybe here explain between the Hermite regime and the approximation regime\nWe can write\n$$\\log(\\lVert \\bold{b_i}^*\\rVert) = \\frac{d - 1 - 2i}{2}\\log(\\alpha_\\beta) + \\frac{1}{d}\\log(Vol(\\Lambda))$$where $\\alpha_\\beta$ is the slope under the geometric assumption that can be calculated from the gaussian assumption as\n$$\\alpha_\\beta = \\sqrt{\\frac{d}{2\\pi e}}^\\frac{2}{\\beta - 1}$$This result from [6] is reasonably accurate only if $d\\gg\\beta$ and $\\beta \u0026gt; 50$, which is why we will use fixed estimates for small dimensions (also, small dimension can be directly solved by an exact solver).\nCost of BKZ # Costing BKZ means having a good idea of the impact of the block-size on the quality of our reduced basis. For this, we could either make the approximation $\\delta_\\beta \\approx \\sqrt \\alpha_\\beta$ or use the following limit defined in [9] that works well for $\\beta \u0026gt; 50$ and typical $d$ used in cryptography [6]. $$\\lim_{\\beta\\rightarrow\\infty}\\delta_\\beta = (\\frac{\\beta}{2\\pi e}(\\pi\\beta)^\\frac{1}{\\beta})^\\frac{1}{2(\\beta - 1)}$$and write for SIS\n$$\\lVert \\bold{b_1} \\rVert \\approx \\delta_\\beta^{d-1} Vol(\\Lambda)^{\\frac{1}{d}}$$ Costing BKZ as a whole is complicated because we do not know how many tours we will have to run, which means we don\u0026rsquo;t really know in advance the number of SVP-Oracle calls we will have to make. Furthermore, many improvements on plain BKZ have been made when some techniques are used as a subroutine for the oracle (for example extreme pruning in the context of enumeration), which makes security estimates done via lattice reduction very sensitive to many factors. Also, local preprocessing techniques have been introduced as part of the algorithm in a variant of BKZ known as progressive BKZ. To make our tool comparable to the lattice estimator by [10], we will follow the same simplifying assumption and consider a consistent 8 tours of BKZ. This makes sense following experimental results that showed that most progress is made in the 7-9 first tours. We will then use: $$cost = \\tau \\cdot d \\cdot T_{SVP}$$ where the number of tours we do $\\tau$ is considered to be 8. The number of times the SVP oracle is called per tour, which is about the dimension of the lattice d The cost of the SVP oracle is $T_{SVP}$ Refinements # The first GSA lie # If we think more about the way we slide the block through the matrix during lattice reduction, we can easily come to the conclusion that something different must happen at the end. Indeed, the GSA assumptions ignores what happens for the last $d-\\beta$ coordinates. This last block is in fact HKZ reduced and we can therefore adapt the tail of our assumption leading to Tail-adapted GSA (TGSA).\nLet us use the following definition for the HKZ-shape in dimension d. For $i=0, \\ldots , d-1:$\n$$ h_i = \\log\\left(\\sqrt{\\frac{d - i}{2\\pi e}}\\right) - \\frac{1}{d-1} \\sum_{j \u003c i} h_j, $$Now using this in modifying our BKZ GSA assumed shape we can get the following two parts equation:\n$$ \\log\\left(\\|b_i^*\\|\\right) = \\begin{cases} \\frac{d - 1 - 2i}{2} \\log(\\alpha \\beta) + s, \u0026 \\text{for } 0 \\leq i \\leq d - \\beta, \\\\ h_{i-(d-\\beta)} + \\log\\left(\\|b_{d-\\beta}^*\\|\\right) - h_0, \u0026 \\text{for } d-\\beta \\leq i \u003c d. \\end{cases} $$ The second GSA lie and Z(T)GSA # Geometric series assumptions (tail-adapted or not) have been shown to be a bad choice for small dimensions (think 50 and below) and when $d$ is a multiple of $\\beta$. Furthermore, these assumptions only give an estimate about the size after the algorithm is finished and not the evolution of vectors through it. We will here just mention that an estimator has been introduced in [3] to take these observations into accounts. Thanks to its implementation in FPyLLL, we will make use of it to calculate the cost of BKZ for small values (by simply hardcoding the needed values).\nBecause we will work with q-ary lattices, they will always contain vector $(q, 0, \\ldots, 0)$ and its permutation. These vectors can be considered short in certain circonstances and shorter that what GSA would predict. A ZGSA assumption adapted to such lattices is possible, but it remains unsure what such an assumption would look like on block reduction algorithms.\nBKZ 2.0 improvements # BKZ 2.0 [3] introduces several enhancements to the traditional BKZ algorithm, improving its efficiency and the quality of lattice reduction. Here, we summarize the key improvements:\nIntroduction of Pruning:\nBKZ 2.0 incorporates sound pruning and extreme pruning, techniques introduced by Gama, Nguyen, and Regev. These pruning methods reduce the size of the enumeration tree by removing branches with negligible probability of success. Preprocessing of Local Blocks:\nBKZ 2.0 ensures local bases are better reduced than standard LLL-reduction before enumeration. This preprocessing step reduces the cost of enumeration by improving the quality of the local basis. For a local projected lattice \\( L[j,k] \\), preprocessing increases the volumes of projected lattices \\( L[k-d+1,k] \\), reducing the size of the enumeration tree. Optimizing the Enumeration Radius:\nBKZ 2.0 optimizes the initial radius \\( R \\) for enumeration to avoid unnecessary computation. The optimized radius is calculated as: $$ R = \\min\\left(\\sqrt{\\gamma} \\cdot \\text{GH}(L[j,k]), \\|b_j^*\\|\\right), $$ where \\( \\text{GH}(L[j,k]) \\) is the Gaussian heuristic for the local block and \\( \\gamma \\approx 1.1 \\). Simulation for High Block Sizes:\nBKZ 2.0 predicts output quality and running time for high block sizes (\\( \\beta \\geq 50 \\)) through a simulation algorithm. This includes: Predicting the Gram-Schmidt sequence \\( \\|b_i^*\\| \\) during BKZ 2.0 reduction. Estimating the block sizes required to achieve a target Hermite factor. P. Nguyen,\u0026#32;Hermite’s constant and lattice algorithms,\u0026#32;in The LLL algorithm: Survey and applications,\u0026#32;Springer, 2009, pp. 19–69.P. Nguyen and\u0026#32;B. Vallee,\u0026#32;The LLL algorithm.\u0026#32;Springer, 2010.\u0026#32;Y. Chen and\u0026#32;P. Nguyen,\u0026#32;BKZ 2.0: Better lattice security estimates,\u0026#32;In Proc. International conference on the theory and application of cryptology and information security,\u0026#32;2011, pp. 1–20.\u0026#32;N. Gama,\u0026#32;N. Howgrave-Graham,\u0026#32;H. Koy, and\u0026#32;P. Nguyen,\u0026#32;Rankin’s constant and blockwise lattice reduction,\u0026#32;In Proc. Advances in cryptology-CRYPTO 2006: 26th annual international cryptology conference, santa barbara, california, USA, august 20-24, 2006. Proceedings 26,\u0026#32;2006, pp. 112–130.\u0026#32;A. Lenstra,\u0026#32;H. Lenstra, and\u0026#32;L. Lovász,\u0026#32;Factoring polynomials with rational coefficients,\u0026#32;Mathematische annalen,\u0026#32;vol. 261,\u0026#32;pp. 515–534,\u0026#32;1982.\u0026#32;M. Albrecht and\u0026#32;L. Ducas,\u0026#32;Lattice attacks on NTRU and LWE: A history of refinements,\u0026#32;Cryptology ePrint Archive,\u0026#32;2021.\u0026#32;N. Gama and\u0026#32;P. Nguyen,\u0026#32;Predicting lattice reduction,\u0026#32;In Proc. Advances in cryptology–EUROCRYPT 2008: 27th annual international conference on the theory and applications of cryptographic techniques, istanbul, turkey, april 13-17, 2008. Proceedings 27,\u0026#32;2008, pp. 31–51.\u0026#32;N. Gama and\u0026#32;P. Nguyen,\u0026#32;Finding short lattice vectors within mordell’s inequality,\u0026#32;In Proc. Proceedings of the fortieth annual ACM symposium on theory of computing,\u0026#32;2008, pp. 207–216.\u0026#32;Y. Chen,\u0026#32;Reduction de reseau et securite concrete du chiffrement completement homomorphe.\u0026#32;Paris 7, 2013.\u0026#32;M. Albrecht,\u0026#32;R. Player, and\u0026#32;S. Scott,\u0026#32;On the concrete hardness of learning with errors.\u0026#32;Cryptology ePrint Archive, Paper 2015/046, 2015.\u0026#32;[Online]. Available: https://eprint.iacr.org/2015/046\u0026#32; "},{"id":2,"href":"/lattirust_estimator.io/docs/cost-models/","title":"Cost Models","section":"Docs","content":" Cost Models # Given the simplified cost for BKZ behaviour that we consider $cost = \\tau \\cdot d \\cdot T_{SVP}$, we still need to define the cost of the SVP solver. Sieving and enumeration are the two common exact strategies to find the shortest non-zero vector in a lattice.\nSieving # Sieving algorithms for SVP work by iteratively refining a large set of lattice vectors to obtain progressively shorter vectors until the shortest one is found. In its basic form, the sieving process starts by generating a large set of random lattice vectors, often called a \u0026ldquo;cloud.\u0026rdquo; Pairs of vectors are then combined (usually by subtracting them) to produce shorter vectors, which are then added back to the cloud if they meet certain criteria. This process continues until the vectors in the cloud converge towards the shortest lattice vector. Modern sieving methods, like the GaussSieve or ListSieve, have been optimized to handle higher-dimensional lattices by limiting pairwise vector interactions, which reduces computational complexity. Sieving requires storing a large number of lattice vectors and, therefore, can be memoryintensive, especially as the lattice dimension grows. Here we present a simplified graphs of selecting two vectors out of a cloud, taking the difference between two vectors and looking what is the smaller between the three vectors.\nSmall example of the sieving logic. In terms of cost, sieving algorithm can solve the SVP in a lattice of dimension d in $2^{O(\\beta)}$ time but at the cost of a much higher memory usage $2^{O(\\beta)}$ than enumeration. Our estimator will disregard memory usage to focus on time cost.\nThe following sieving estimates are available in the estimator:\nName Reference Cost Regime BDGL-sieve [1] $2^{0.292\\beta + 16.4}$ big $\\beta$ $2^{0.387\\beta + 16.4}$ small $\\beta$ classical Q-sieve [2] [3] $2^{0.265\\beta}$ quantum ADPS-sieve [2] $2^{0.292\\beta}$ classical BGJ-sieve [4] $2^{0.311\\beta}$ classical ChaLoy-sieve [5] $2^{0.257\\beta}$ quantum Cost of sieving SVP solvers As a very high overview, we give a list of amelioration brought by each kind of sieving solver considered :\nBDGL-sieve introduces a novel approach to improving lattice sieving efficiency through locality-sensitive filtering (LSF), which significantly reduces the time complexity of solving the shortest vector problem (SVP) on high-dimensional lattices. By utilizing spherical caps as filters and leveraging structured random product codes for efficient decoding, the method achieves an asymptotic time complexity of \\(2^{0.292n + o(n)}\\), outperforming previous approaches like spherical locality-sensitive hashing (LSH).\nADPS-sieve presents sintroduces quantum speedups through Grover’s algorithm, reducing the classical sieving complexity from \\(2^{0.415b}\\) to \\(2^{0.292b}\\). Additionally, it leverages structured lattice representations to optimize memory usage and sieve iterations. This method focuses on balancing theoretical improvements with practical implementations, providing a framework for faster and more scalable sieving for lattice-based cryptographic challenges.\nBGJ-sieve significantly reduces the time complexity to \\(2^{0.292d + o(d)}\\) through algorithmic optimizations. Key contributions include the introduction of hypersphere partitioning to reduce vector comparison costs, the use of nearby vector replacement to iteratively refine the list of vectors towards shorter ones, and a probabilistic framework that efficiently handles high-dimensional spaces.\nChaLoy-sieve introduces a quantum algorithm based on quantum random walks that improves the asymptotic complexity of solving the Shortest Vector Problem (SVP). The key innovation is replacing Grover\u0026rsquo;s algorithm with a quantum random walk, enabling faster resolution of reducible vector pairs. This enhancement reduces the heuristic running time from \\(2^{0.2653d + o(d)}\\) to \\(2^{0.2570d + o(d)}\\), while optimizing resource usage. Notably, it decreases quantum memory requirements to \\(2^{0.0495d}\\) and quantum RAM to \\(2^{0.0767d}\\).\nThe Dimension for free # In recent advancements, Ducas introduces a significant improvement in lattice sieving algorithms through the \u0026ldquo;dimension for free\u0026rdquo; technique [6]. This approach leverages the observation that short lattice vectors generated during sieving in lower dimensions can be reused to accelerate sieving in higher dimensions. By projecting and reusing these vectors across multiple dimensions, the algorithm effectively reduces the computational overhead without compromising accuracy. This method achieves a practical speedup of up to 10x in mid-range dimensions (e.g., 70–80). Our estimators will automatically provide this improvement by adapting the size of the needed lattice reduction computation. In practice, we can reduce the SVP problem of dimension $n$ to solving an SVP problem of dimension $n-d$ as long as $d$ is $\\Theta(\\frac{n}{\\log(n)})$. The paper mentions a concrete prediction of $d \\approx \\frac{n\\ln(4/3)}{\\ln(n/2\\pi e)}$.\nEnumeration # Enumeration algorithms systematically search through lattice points in a controlled way, typically by traversing lattice vectors within a fixed radius from the origin. They rely on a recursive process to explore potential candidate vectors within a \u0026ldquo;search region,\u0026rdquo; using techniques to prune paths that are unlikely to lead to the shortest vector. Enumeration is typically carried out with the help of a basis that has been reduced (made close to orthogonal), as this greatly improves efficiency by minimizing the number of candidate paths. Unlike sieving, enumeration methods are deterministic and guarantee finding the shortest vector by systematically exploring all feasible paths. The efficiency of enumeration depends strongly on the quality of the lattice basis. Preprocessing steps like BKZ (Block Korkine-Zolotarev) reduction can make enumeration significantly faster by transforming the basis to be more suitable for search. Enumeration is often practical for lower-dimensional lattices or when a high degree of accuracy is needed, but it tends to be less efficient than sieving in high dimensions due to its exponential complexity. Hereafter we present a simplified version of how enumeration could progress. Given a basis b1 and b2, we project one of the two and use it to make a grid spaced by the length of the projection. Using this 1-dim problem we can lift back up to the 2-d problem by selecting all points in the Radius that are on the grid.\nSmall example of the enumeration logic. In terms of cost, enumeration can solve SVP in a lattice of dimension d in $2^{O(\\beta \\log \\beta)}$ time and space poly($\\beta$).\nThe following enumeration estimates are available:\nName Reference Cost Regime Lotus [4] [7] $2^{0.125\\beta\\log\\beta -0.755\\beta + 22.74}$ classical CheNgue-enum (BKZ 2.0) [8] $2^{0.27\\beta\\log\\beta -1.019\\beta + 2.254}$ classical ABF-enum [9] $2^{0.184\\beta\\log\\beta - 0.995\\beta + 22.25}$ small $\\beta$ $2^{0.125\\beta\\log\\beta-0.547\\beta+16.4}$ big $\\beta$ classical ABF-Q-enum [9] $2^{0.0625\\beta\\log\\beta}$ quantum ABLR-enum [10] $2^{0.184\\beta\\log\\beta - 1.077\\beta + 35.12}$ small $\\beta$ $2^{0.125\\beta\\log\\beta-0.655\\beta+31.84}$ big $\\beta$ classical Here are some plots to better visualize enumeration costs.\nCost of enumeration SVP solvers As a very high overview, we give a list of amelioration brought by each kind of enumeration solver considered :\nABF-Enum brings introduces the concept of extended preprocessing. This approach decouples the preprocessing and enumeration contexts, enabling the use of higher-dimensional sublattices for preprocessing while optimizing the enumeration cost. By leveraging the enhanced geometric properties of SDBKZ-reduced bases, ABF-Enum achieves faster enumeration with reduced computational complexity, pushing the boundaries of lattice reduction efficiency. This innovation forms the foundation for achieving improved trade-offs between time complexity and quality of solutions in enumeration-based solvers.\nBKZ 2.0 (ChNgue) optimizes the Blockwise Korkine-Zolotarev (BKZ) algorithm. Key enhancements include the incorporation of sound pruning, a sophisticated technique to reduce enumeration cost while maintaining quality, and preprocessing of local blocks to improve the efficiency of enumeration subroutines. Additionally, optimized enumeration radii enable faster convergence by leveraging improved heuristics. These innovations collectively improve both the speed and output quality of lattice reduction, making BKZ 2.0 a new benchmark for practical lattice algorithms.\nABLR-Enum innovates by combining relaxed pruning with extended preprocessing strategies. These enhancements allow enumeration algorithms to achieve exponential speed-ups while maintaining high-quality reductions. By relaxing the search radius and optimizing enumeration parameters, the approach efficiently balances time and output quality. The extended preprocessing step leverages higher-dimensional sublattices to improve enumeration performance, marking a significant step forward in practical and scalable lattice reduction.\nThe LOTUS cryptosystem enhances lattice enumeration techniques through optimized pruning strategies, which reduce the computational effort required for solving lattice problems like bounded distance decoding (BDD). By refining cost estimation methods for lattice reduction and enumeration.\nMethod comparison # Cost of all solvers We can see that sieving performs better overall when the dimension of the SVP problem get big, which is the expected behavior. For this reason, we encourage making security estimates with sieving as an underlying SVP solver.\nReferences # A. Becker,\u0026#32;L. Ducas,\u0026#32;N. Gama, and\u0026#32;T. Laarhoven,\u0026#32;New directions in nearest neighbor searching with applications to lattice sieving,\u0026#32;In Proc. Proceedings of the twenty-seventh annual ACM-SIAM symposium on discrete algorithms,\u0026#32;2016, pp. 10–24.\u0026#32;E. Alkim,\u0026#32;L. Ducas,\u0026#32;T. Pöppelmann, and\u0026#32;P. Schwabe,\u0026#32;Post-quantum key Exchange—A new hope,\u0026#32;In Proc. 25th USENIX security symposium (USENIX security 16),\u0026#32;2016, pp. 327–343.\u0026#32;T. Laarhoven,\u0026#32;M. Mosca, and\u0026#32;J. Van De Pol,\u0026#32;Finding shortest lattice vectors faster using quantum search,\u0026#32;Designs, Codes and Cryptography,\u0026#32;vol. 77,\u0026#32;pp. 375–400,\u0026#32;2015.\u0026#32;M. Albrecht,\u0026#32;B. Curtis,\u0026#32;A. Deo,\u0026#32;A. Davidson,\u0026#32;R. Player,\u0026#32;E. Postlethwaite,\u0026#32;F. Virdia, and\u0026#32;T. Wunderer,\u0026#32;Estimate all the ${$LWE, NTRU$}$ schemes!,\u0026#32;In Proc. Security and cryptography for networks: 11th international conference, SCN 2018, amalfi, italy, september 5–7, 2018, proceedings 11,\u0026#32;2018, pp. 351–367.\u0026#32;A. Chailloux and\u0026#32;J. Loyer,\u0026#32;Lattice sieving via quantum random walks,\u0026#32;In Proc. Advances in cryptology–ASIACRYPT 2021: 27th international conference on the theory and application of cryptology and information security, singapore, december 6–10, 2021, proceedings, part IV 27,\u0026#32;2021, pp. 63–91.\u0026#32;L. Ducas,\u0026#32;Shortest vector from lattice sieving: A few dimensions for free,\u0026#32;In Proc. Annual international conference on the theory and applications of cryptographic techniques,\u0026#32;2018, pp. 125–145.\u0026#32;T. Le Trieu Phong,\u0026#32;Y. Aono, and\u0026#32;S. Moriai,\u0026#32;Lotus.\u0026#32;Technical report, National Institute of Standards; Technology, 2017 …, 2017.\u0026#32;Y. Chen and\u0026#32;P. Nguyen,\u0026#32;BKZ 2.0: Better lattice security estimates,\u0026#32;In Proc. International conference on the theory and application of cryptology and information security,\u0026#32;2011, pp. 1–20.\u0026#32;M. Albrecht,\u0026#32;S. Bai,\u0026#32;P. Fouque,\u0026#32;P. Kirchner,\u0026#32;D. Stehlé, and\u0026#32;W. Wen,\u0026#32;Faster enumeration-based lattice reduction: Root hermite factor time,\u0026#32;In Proc. Annual international cryptology conference,\u0026#32;2020, pp. 186–212.\u0026#32;M. Albrecht,\u0026#32;S. Bai,\u0026#32;J. Li, and\u0026#32;J. Rowell,\u0026#32;Lattice reduction with approximate enumeration oracles: Practical algorithms and concrete performance,\u0026#32;In Proc. Annual international cryptology conference,\u0026#32;2021, pp. 732–759.\u0026#32; "},{"id":3,"href":"/lattirust_estimator.io/docs/sis/","title":"SIS","section":"Docs","content":" Short Integer Solution problem (SIS) # Let us look at two different ways to define m-dimensional q-ary lattices from a matrix \\(\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}\\). We cran write:\n$$\\Lambda_q(\\bold{A}) = \\{\\bold{y} \\in \\mathbb{Z}^w : \\bold{y} = \\bold{A}^T\\bold{s} \\text{ mod q }\\text{ for some } \\bold{s} \\in \\mathbb{Z}^h\\}$$$$\\Lambda^T_q(\\bold{A}) = \\{\\bold{y} \\in \\mathbb{Z}^w : \\bold{A}\\bold{y} = \\bold{0} \\text{ mod q }\\}$$The first lattice, \\( \\Lambda_q(\\boldsymbol{A}) \\), is generally referred to as the primal problem and is closely related to the Learning With Errors (LWE) problem. Solving the LWE problem involves finding a short vector in \\( \\Lambda_q(\\boldsymbol{A}) \\).\nLWE Problem Statement # Given:\nA matrix \\( \\mathbf{A} \\in \\mathbb{Z}_q^{h \\times w} \\), where entries are chosen uniformly at random, A secret vector \\( \\mathbf{s} \\in \\mathbb{Z}_q^w \\), chosen uniformly at random, An error vector \\( \\mathbf{e} \\in \\mathbb{Z}_q^h \\), where each entry is drawn from an error distribution \\( \\chi \\) (e.g., a discrete Gaussian distribution), A vector \\( \\mathbf{b} = \\mathbf{A} \\mathbf{s} + \\mathbf{e} \\mod q \\). To find:\nIn the search version: The secret vector \\( \\mathbf{s} \\in \\mathbb{Z}_q^w \\), In the decision version: Distinguish between: Vectors \\( (\\mathbf{A}, \\mathbf{b}) \\) generated as described above, and Vectors \\( (\\mathbf{A}, \\mathbf{b}) \\) where \\( \\mathbf{b} \\) is chosen uniformly at random from \\( \\mathbb{Z}_q^h \\). We often assume that the error distribution \\( \\chi \\) has small standard deviation compared to \\( q \\).\nThe second lattice, \\( \\Lambda_q^T(\\boldsymbol{A}) \\), is generally referred to as the dual problem and is closely related to the Short Integer Solution (SIS) problem. Solving the SIS problem involves finding a short vector in \\( \\Lambda_q^T(\\boldsymbol{A}) \\).\nSIS Problem Statement # Given:\nA matrix \\( \\boldsymbol{A} \\in \\mathbb{Z}_q^{h \\times w} \\), where entries are uniformly sampled from \\( \\mathbb{Z}_q \\), An integer bound \\( \\beta \u003e 0 \\). To find:\nA nonzero vector \\( \\boldsymbol{y} \\in \\mathbb{Z}^w \\) such that: $$ \\boldsymbol{A} \\boldsymbol{y} = \\boldsymbol{0} \\mod q \\quad \\text{and} \\quad \\|\\boldsymbol{y}\\| \\leq \\beta $$ We often assume that the entries of \\( \\boldsymbol{A} \\) are uniformly distributed over \\( \\mathbb{Z}_q \\).\nFormal definitions # Firstly, let us assume that q is always prime and that $w\u0026gt;h$. We can state the following definitions.\nDefinition 1 : We define \\(SIS(h, w, q, \\beta, p)\\) as follows. Given \\(\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}\\) find the short vector \\(\\bold{s} \\in \\mathbb{Z}^w\\) where \\(0 \u0026lt; \\lVert s \\rVert_p \\leq \\beta\\) and $q=\\text{poly}(h)$, $m=\\text{poly}(h)$, $\\beta=\\text{poly}(h)$.\nWe can note that the problem becomes trivial as soon as \\(\\beta \\geq q\\), no matter the norm used. Also, for the problem not to be vacuous (no possible solutions), we need the select $\\beta$ big enough. An often used bound is $\\beta \u0026gt; \\sqrt{h\\log(q)}$\nDefinition 2: The inhomogeneous SIS problem is called \\(ISIS(h, q, w, \\beta, p, t)\\) and is defined as follows. Given \\(\\bold{A} \\in \\mathbb{Z}_q^{h \\times w}\\) and a target vector \\(\\bold{t} \\in \\mathbb{Z}_q^h\\), find a short vector \\(\\bold{s} \\in \\mathbb{Z}^w\\) such that \\(\\bold{A} \\bold{s} = \\bold{t} \\mod q\\) and \\(0 \u0026lt; \\lVert \\bold{s} \\rVert_p \\leq \\beta\\). Also here $q=\\text{poly}(h)$, $m=\\text{poly}(h)$, $\\beta=\\text{poly}(h)$.\nLemma 1: SIS and ISIS are considered asymptotically equivalent problem (a reduction exists from one to the other) [1].\nLemma 2: Solving SIS on $A^T$ directly allows to solve LWE on $A$. They are also considered asymptotically equivalent problems. A reduction exists from SIS to LWE. In the other direction, a quantum reduction exists.\nThe following theorems are the fundamental theorems that allows us to relate an SIS instance to a known very hard problem (SVP-like).\nTheorem 1: For any polynomially bounded $w$, $\\beta = \\text{poly}(h)$, and any prime $q \\geq \\beta \\cdot \\omega(\\sqrt{h \\log h})$, the average-case problems $SIS(h, q, w, \\beta)\\text{ and } ISIS(h, q, w, \\beta)$ are at least as hard as approximating the $SIVP$ problem (among others) in the worst case within a factor of $\\gamma = \\beta \\cdot \\tilde{O}(\\sqrt{n})$ [2].\nThis result has been improved for smaller value of the q parameters (nearly equal to the length bound $\\beta$).\nTheorem 2: For any polynomially bounded $w$, $\\beta = \\text{poly}(h)$, and any prime $q \\geq \\beta \\cdot h^\\delta$, $\\delta\u0026gt;0$ some constant, the average-case problems $SIS(h, q, w, \\beta)\\text{ and } ISIS(h, q, w, \\beta)$ are at least as hard as approximating the $SIVP$ problem (among others) in the worst case within a factor of $\\gamma = \\beta \\cdot \\tilde{O}(\\sqrt{n})$ [3].\nThis means that to estimate the security given by an SIS instance on which a cryptographic primitive relies, we can estimate the security of solving(or attacking) an SVP-instance. In the following we will present an high overview of possible attacks, and the one we choose to base our security estimates on.\nAttacks on SIS # Lattice reduction based attacks # Firstly, let\u0026rsquo;s mention lattice reduction that we have already broadly covered, so we use lattice reduction to directly find short vectors. This is the approach we take when estimating the hardness of an SIS instance and we will provide further details on the exact steps for the L-2 and L-infinity norms later on.\nCombinatorial attacks # Combinatorial attacks to solve were proposed in [4]. They work as follows, given matrix $\\bold{A} \\in \\mathbb{Z}_q^{h \\times w}$ and the length bound $b$:\nDivide Columns into Groups: Split the columns of \\( \\boldsymbol{A} \\) into \\( 2^k \\) groups, each containing \\( \\frac{w}{2^k} \\) columns, where \\( k \\) is a parameter to be determined. Generate Initial Lists: For each group, construct a list containing all linear combinations of the columns with coefficients in \\( \\{-b, \\dots, b\\} \\). Each list contains \\( L = (2b + 1)^{w / 2^k} \\) vectors in \\( \\mathbb{Z}_q^h \\). Combine Lists in Pairs: Iteratively combine the lists in pairs. For two lists, compute all sums \\( \\boldsymbol{x} + \\boldsymbol{y} \\), retaining only those vectors whose first \\( \\log_q L \\) coordinates are zero. This reduces the size of the resulting list to approximately \\( L \\). Repeat Combination: Continue combining lists until a single list remains after \\( k \\) iterations. The final list contains vectors that are zero in their first \\( k \\cdot \\log_q L \\) coordinates. Extract the Short Vector: The final list contains vectors that are zero in all but their last \\( h - k \\cdot \\log_q L \\approx \\log_q L \\) coordinates. With appropriate parameter choice k, the list is expected to include the all-zero vector, which is a combination of the columns of $\\bold{A}$ bounded by $b$ so we can expect to find a short vector. Algorithms based on this combinatorial attacks but on LWE have also been proposed like BKW [5] and Coded-BKW [6]. Combinatorial attacks work well for small dimensions but become quickly unfeasible for larger dimensions. However, one good aspects of combinatorial methods is that they actually exploit the large number of columns \\( w \\) in \\( \\boldsymbol{A} \\), which lattice reduction do not.\nVia LWE # In the case of solving SIS through LWE, several additional attacks can be considered, such as bounded distance decoding (BDD) [7], Arora-Ge attacks [8], and meet-in-the-middle (MITM) [9] approaches. BDD focuses on finding the closest lattice point to a given target, leveraging the fact that LWE can be reduced to a closest vector problem (CVP) in certain cases. Arora-Ge attacks use algebraic techniques to solve LWE by reducing it to a system of polynomial equations, though their practicality diminishes with high noise. MITM attacks exploit combinatorial techniques to reduce the effective complexity of solving LWE by balancing the time and memory trade-offs. As SIS is our focus, we will not delve deeply into these methods.\nIn the following analysis, we will present the concrete method used to estimate the security of an SIS instance. We will separate the use of the euclidean norm and the infinity norm, using the theory we built in the previous sections (lattice reduction and cost models). What we actually reduce te security to is an H-SVP solver.\nL2 norm strategy # For the L2 norm strategy, we follow the steps depicted in [4] and [10]. In the following you can assume all log are base 2, except mentioned otherwise. As a first step, we ensure the parameters given don\u0026rsquo;t solve trivially or actually accept solutions by checking that $\\beta \u0026lt; q$ and $\\beta \\geq w\\log(q)$.\nFinding the optimal lattice shape for reduction # Given a q-ary lattice \\(\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}\\), we can then say with high probability that the rows of $\\bold{A}$ are independent over $\\mathbb{Z}_q$. As a result of this, the lattice $\\Lambda^T_q(\\bold{A})$ has $q^{w-h}$ points in $\\mathbb{Z}^w_q$. This leads to the volume or determinant of the matrix to be $Vol(\\Lambda^T_q(\\bold{A})) = q^{h}$. Using the gaussian heuristic from the lattice reduction section, we can express\n$$\\lambda_1(\\Lambda^T_q(\\bold{A})) \\approx q^\\frac{h}{w}\\sqrt{\\frac{w}{2\\pi e}}$$as an estimate of the lenght of the smallest vector.\nIt has been experimentally observed that the length of the vector obtained by the best known lattice reduction algorithms on a random w-dimensional q-ary lattice is close to $\\min(q, q^\\frac{h}{w}\\delta^w)$. We can also observe that increasing the $w$ parameter does not make the problem any harder. In fact, we can fix the parameter $w$ by completely letting go a certain number of columns. By plotting $q^{\\frac{h}{w}}\\delta^w$ as a function of w, the minimum as been determined to be $2^{2\\sqrt{h\\log q \\log \\delta}}$ for $w = \\sqrt{h\\log q / \\log \\delta}$. So we will always do a lattice reduction over a matrix of size $h\\times w\u0026rsquo;$, where\n$$w' = \\sqrt{\\frac{h\\log q}{\\delta}}$$Indeed, for smaller w the lattice becomes too sparse and does not contain enough vectors to have small ones, and bigger w actually prevents lattice reduction to perform optimally. This leads us to a shortest vector found of length $\\min(q, 2^{2\\sqrt{h\\log q \\log \\delta}})$. In the calculation for the optimal $w\u0026rsquo;$, we use the theoretical value for $\\delta$ and replace the bound of the smallest vector by $\\beta$ because we want a vector that is smaller or equal to $\\beta$.\n$$\\lambda_1(\\Lambda^T_q(\\bold{A})) = q^\\frac{h}{w}\\delta^w \\Rightarrow \\beta = q^\\frac{h}{w}\\delta^w $$$$\\log \\beta = w \\log \\delta + \\frac{h}{w}\\log q \\Rightarrow \\log \\delta = \\frac{\\log \\beta}{w} - \\frac{h\\log q}{w^2}$$Replacing w by w\u0026rsquo; we get the theoretical\n$$ \\log\\delta = \\frac{\\log^2\\beta}{4h\\log q} $$ Solve for the required hermite factor # Now that we have the optimal dimension on which to apply our lattice reduction via BKZ, we need to define what the best block-size would be. We know from previously that BKZ achieves\n$$\\beta \\approx \\delta_\\beta^{w'-1} Vol(\\Lambda)^{\\frac{1}{w'}}$$and by playing around with the equation, we know that our target hermite factor will be\n$$\\beta^\\frac{1}{w'-1} = \\delta Vol(\\Lambda)^{\\frac{1}{w'(w' - 1)}} \\Rightarrow \\delta \\approx \\beta \\frac{1}{Vol(\\Lambda)^{1/w'}}^\\frac{1}{w'-1}$$$$\\log\\delta = \\frac{1}{w'-1}(\\log\\beta - \\frac{h}{w'}\\log q)$$We check that this found $\\delta$ makes sense by checking that it is bigger than one. Now that we know what we aim for in terms of hermite factor, the next question is what kind of block-size gives us such a guarantee. For this, we follow [11] and retake their manually computed $\\delta_\\beta$ values for small $\\beta$ under 40 (that were computed using fpyll) and for bigger block-sizes, we use\n$$\\lim_{\\beta\\rightarrow\\infty}\\delta_\\beta = (\\frac{\\beta}{2\\pi e}(\\pi\\beta)^\\frac{1}{\\beta})^\\frac{1}{2(\\beta - 1)}$$Once we know the optimal $\\beta$, we can check that it also makes sense by checking it is under our matrix size (so under $w\u0026rsquo;$). If it is, we can go ahead and estimate the actual cost of running BKZ for block-size $\\beta$ on a matrix of dimension $\\bold{A}\\in \\mathbb{Z}_q^{h \\times w\u0026rsquo;}$.\nEstimating the security by estimating the BKZ cost # $$cost = \\tau \\cdot d \\cdot T_{SVP}$$ where\nthe number of tours we do $\\tau$ is considered to be 8. The number of times the SVP oracle is called per tour, which is about the dimension of the lattice The cost of the SVP oracle is $T_{SVP}$ The cost of the SVP oracle is defined by the internal cost that the user will select.\nL-inf norm strategy # As in [11], we separate between the Matzov and the Kyber analysis. Both however follow the same high-level strategy. First, we evaluate the security for the L2-bound as explained previously.This will act as a lower bound on the hardness of our infinity bound problem. The strategy is then to analyze the probability of obtaining a vector that respects the infinity bound constraints on all coordinates. When $\\sqrt{w}\\beta \u0026lt; q$ we apply the analysis in [12] and when $\\sqrt{w}\\beta \\geq q$, we apply the analysis in [13]. The attack is composed on 2 parts. First, we use lattice reduction to find many short vectors of the lattice. Let\u0026rsquo;s imagine we receive a secret $\\bold{y}$ such that $\\bold{y} = \\bold{A}\\bold{x}$ where $\\bold{x}$ is short. This is in fact an ISIS instance. Our goal in the second step is to determine with statistical tools if $\\bold{y}$ was sampled from the uniform distribution or computed using the secret $\\bold{x}$. Imagining for have a statistical distinguisher between the 2 distribution, the attacker can now for each coordinate of the secret $\\bold{x}$ guess its value and use the distinguisher to determine if it is good.\nMatzov analysis # The improvements made is that first, the attacker can iterate over several coordinates of the secret at the same time. This lets us recover more coordinate and reduces the dimension of the lattice. The second improvement is the use of the Fast Fourier Transform (FFT) as a dstinghuishing algorithm, which allows to check all guesses simultaneously with a single FFT computation. The computation is actually done in a modulus p smaller than q, with the argument being that since the secret is short, we do not introduce bug errors by doing so. The steps can be summarized in:\nGenerate many pairs of short vectors $(\\bold{x}, \\bold{y})$. This step is improved by using the dimension for free optimisation for all SVP calls excepts the last and using a different block-size for the last call. Enumerate over guesses of the secret. For each guesses, use the FFT dstinguisher. For a detailed overview of the attack, we encourage the reader to go to [12] as we will only dig into the cost of the attack.\nCost # Kyber analysis # The tradeoffs # The search function # "},{"id":4,"href":"/lattirust_estimator.io/docs/rsis-msis/","title":"Module SIS and Ring SIS","section":"Docs","content":" Compact versions of SIS # In practice, basic SIS leads to large key sizes and large parameters overall. Thinking about $\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}$ with $m \u0026gt; n$ leads to at least quadratic complexity. This is why more compact version have been introduced like Ring-SIS and Module-SIS. By using such underlying structures, we are able to obtain much smaller key sizes in the schemes that use an SIS instance as the basis for their security. As a refresher on the module and ring structure the reader can consult [1]. Consider that we choose some $h = 2^k$ and that we choose the columns of $\\bold{A}$ to be elements of the ring $\\mathbb{Z}_q / \\langle x^h + 1 \\rangle$, if $h$ is a power of two it is irreducible over the rationals, then we need to precise less elements to describe the same SIS instance. Using a module, we can get an even compacter structure.\nRing SIS # Definition:[2]\nWe define the Ring Short Integer Solution (Ring-SIS) problem as follows $RSIS(h, w, q, \\beta, p)$. Let:\n\\( R_q = \\mathbb{Z}_q / \\langle x^h + 1 \\rangle \\), where \\( h \\) is a power of two (ensuring \\( x^h + 1 \\) is irreducible over \\( \\mathbb{Q} \\)), Given $a_1, \\ldots, a_w$ in R_q, chosen independently from the uniform distribution, find $s_1, \\ldots, s_w \\in R$ such that:\n$$ \\sum_{i=1}^{w}a_i s_i = 0 \\pmod{q} \\text{ and } 0 \u003c \\|\\mathbf{s}\\|_p \\leq \\beta $$where $\\bold{s} = (s_1, \\ldots, s_w)^T \\in R^m$.\nEach ring element $r\\in R$ can be interpreted as an $h$-dimensional vector with coefficients $r_i$ such that $r = \\sum_{i=0}^{n-1}r_ix^i$. Now if we compare RSIS to SIS, each matrix element $a_i$ in RSIS actually actually corresponds to the $h\\times h$ nega-circulant matrix. In this setup, R-SIS is just a variant of SIS where $\\bold{A}$ is restricted to being block negacirculant $\\bold{A}= [Rot(a_1)|\\ldots|Rot(a_n)]$ where Rot(b) is defined as\n$$ \\text{Rot}(b) = \\begin{bmatrix} b_0 \u0026 -b_{h-1} \u0026 -b_{h-2} \u0026 \\cdots \u0026 -b_1 \\\\ b_1 \u0026 b_0 \u0026 -b_{h-1} \u0026 \\cdots \u0026 -b_2 \\\\ b_2 \u0026 b_1 \u0026 b_0 \u0026 \\cdots \u0026 -b_3 \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ b_{h-1} \u0026 b_{h-2} \u0026 b_{h-3} \u0026 \\cdots \u0026 b_0 \\end{bmatrix} $$ Reduction to SIS # To estimate the security of a ring SIS instance, we then effectively transform an RSIS instance $RSIS(h, w, q, \\beta, p)$ to an SIS instance $SIS(h, h \\cdot w, q, \\beta, p)$\nModule SIS # Definition:[2]\nWe define the Module Short Integer Solution (Module-SIS) problem as follows $MSIS(h, w, d, q, \\beta, p)$. Let:\n\\( R = \\mathbb{Z}_q / \\langle x^h + 1 \\rangle \\), where \\( h \\) is a power of two, \\( \\mathcal{M} = R^d \\), a rank-\\( d \\) module over \\( R \\), Given $\\bold{a_1}, \\ldots, \\bold{a_w} \\in R_q^d$ chosen independently from the uniform distribution, find $s_1, \\ldots, s_w \\in R$ such that:\n$$ \\sum_{i=1}^{w}\\bold{a_i} s_i = 0 \\pmod{q} \\text{ and } 0 \u003c \\|\\mathbf{s}\\|_p \\leq \\beta $$where $\\bold{s} = (s_1, \\ldots, s_w)^T \\in R^m$.\nEach element $\\bold{a_i}$ can be seen as d coefficients in the ring and as such can be seen as an $h\\cdot d \\times h$ matrix. This leads a module SIS problem to be visualized as such in a standard SIS problem:\n\\[ \\mathbf{A} = \\begin{bmatrix} \\text{Rot}(a_{1,1}) \u0026 \\text{Rot}(a_{1,2}) \u0026 \\cdots \u0026 \\text{Rot}(a_{1,w}) \\\\ \\text{Rot}(a_{2,1}) \u0026 \\text{Rot}(a_{2,2}) \u0026 \\cdots \u0026 \\text{Rot}(a_{2,w}) \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\text{Rot}(a_{d,1}) \u0026 \\text{Rot}(a_{d,2}) \u0026 \\cdots \u0026 \\text{Rot}(a_{d,w}) \\end{bmatrix}, \\]where each block \\(\\text{Rot}(a_{i,j})\\) is a negacyclic matrix defined as:\n\\[ \\text{Rot}(b) = \\begin{bmatrix} b_0 \u0026 -b_{n-1} \u0026 -b_{n-2} \u0026 \\cdots \u0026 -b_1 \\\\ b_1 \u0026 b_0 \u0026 -b_{n-1} \u0026 \\cdots \u0026 -b_2 \\\\ b_2 \u0026 b_1 \u0026 b_0 \u0026 \\cdots \u0026 -b_3 \\\\ \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ b_{n-1} \u0026 b_{n-2} \u0026 b_{n-3} \u0026 \\cdots \u0026 b_0 \\end{bmatrix}. \\] Reduction to SIS # To estimate the security of a module SIS instance, we then effectively transform an MSIS instance $MSIS(h, w, d, q, \\beta, p)$ to an SIS instance $SIS(h \\cdot d, h \\cdot w, q, \\beta, p)$. Also, note that by using $d=1$, we effectively come back to RSIS. Indeed, modules are a generalization of rings.\nReferences # J. Lambek,\u0026#32;Lectures on rings and modules.\u0026#32;American Mathematical Soc., 2009.\u0026#32;A. Langlois and\u0026#32;D. Stehlé,\u0026#32;Worst-case to average-case reductions for module lattices,\u0026#32;Designs, Codes and Cryptography,\u0026#32;vol. 75,\u0026#32;no. 3,\u0026#32;pp. 565–599,\u0026#32;2015.\u0026#32; "},{"id":5,"href":"/lattirust_estimator.io/docs/sis-variants/","title":"SIS variants","section":"Docs","content":" SIS variants # We will now present all SIS problem variants implemented in our tools. Most effectively hand out additional hints to the adversary, while hoping that the problem remains hard. In all cases, we will try to define the SIS variants precisely, give the reduction to the basis SIS problem we use (SIS, RSIS, or MSIS) and also give a rough idea of what kind of scheme the problem variant allowed to construct. While we will try to be as close as possible from the original papers, this (and the entire webpage) could probably still contains mistakes. If you are involved in any of these schemes, feel free to reach out with any corrections. In all cases, we will also show an example of how the variants can be called via the estimator.\nBASIS_rand # [1] Calling an instance # code from estimator\nBASIS_struct # [1] Calling an instance # code from estimator\nISISf # Definition [2]\nCalling an instance # code from estimator\nk-SIS # Definition [3]\nCalling an instance # code from estimator\nk-M-SIS # Definition [4]\nCalling an instance # code from estimator\nk-R-SIS # Definition [4]\nCalling an instance # code from estimator\nk-R-ISIS # Definition [4]\nCalling an instance # code from estimator\nvanishing-SIS # Definition [5]\nCalling an instance # code from estimator\nPRISIS # Definition [6]\nCalling an instance # code from estimator\none-more-ISIS # [7] Calling an instance # code from estimator\nReferences # H. Wee and\u0026#32;D. Wu,\u0026#32;Succinct vector, polynomial, and functional commitments from lattices,\u0026#32;In Proc. Annual international conference on the theory and applications of cryptographic techniques,\u0026#32;2023, pp. 385–416.\u0026#32;J. Bootle,\u0026#32;V. Lyubashevsky,\u0026#32;N. Nguyen, and\u0026#32;A. Sorniotti,\u0026#32;A framework for practical anonymous credentials from lattices,\u0026#32;In Proc. Annual international cryptology conference,\u0026#32;2023, pp. 384–417.\u0026#32;D. Boneh and\u0026#32;D. Freeman,\u0026#32;Linearly homomorphic signatures over binary fields and new tools for lattice-based signatures,\u0026#32;In Proc. International workshop on public key cryptography,\u0026#32;2011, pp. 1–16.\u0026#32;M. Albrecht,\u0026#32;V. Cini,\u0026#32;R. Lai,\u0026#32;G. Malavolta, and\u0026#32;S. Thyagarajan,\u0026#32;Lattice-based snarks: Publicly verifiable, preprocessing, and recursively composable,\u0026#32;In Proc. Annual international cryptology conference,\u0026#32;2022, pp. 102–132.\u0026#32;V. Cini,\u0026#32;R. Lai, and\u0026#32;G. Malavolta,\u0026#32;Lattice-based succinct arguments from vanishing polynomials,\u0026#32;In Proc. Annual international cryptology conference,\u0026#32;2023, pp. 72–105.\u0026#32;G. Fenzi,\u0026#32;H. Moghaddas, and\u0026#32;N. Nguyen,\u0026#32;Lattice-based polynomial commitments: Towards asymptotic and concrete efficiency,\u0026#32;Journal of Cryptology,\u0026#32;vol. 37,\u0026#32;no. 3,\u0026#32;p. 31,\u0026#32;2024.\u0026#32;S. Agrawal,\u0026#32;E. Kirshanova,\u0026#32;D. Stehlé, and\u0026#32;A. Yadav,\u0026#32;Practical, round-optimal lattice-based blind signatures,\u0026#32;In Proc. Proceedings of the 2022 acm sigsac conference on computer and communications security,\u0026#32;2022, pp. 39–53.\u0026#32; "},{"id":6,"href":"/lattirust_estimator.io/docs/estimator-options/","title":"Estimator options and API","section":"Docs","content":" Our tool # Searching for the best parameters on single SIS instances # Searching for the best parameters on multiple SIS instances # TODO\n"}]