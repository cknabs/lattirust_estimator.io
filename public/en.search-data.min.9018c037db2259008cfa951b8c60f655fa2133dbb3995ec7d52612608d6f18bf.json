[{"id":0,"href":"/docs/preliminaries/","title":"Preliminaries","section":"Docs","content":"\rPreliminaries\r#\rLattices\r#\rA lattice is defined as a discrete subgroup in n-dimensional $\\mathbb{R}^n$ space with a periodic structure. It can be represented by a set of linearly independent vectors commonly named the basis of the lattice. If $\\bold{b_1}, \u0026hellip;, \\bold{b_d}$ denote basis vectors we can then describe a lattice by:\n$$\\Lambda(\\bold{b_1}, ..., \\bold{b_d}) = \\{\\sum_{i=1}^{d}x_i\\bold{b_i}: x_i \\in \\mathbb{Z}\\}$$ which is the set of all linear combinations of basis vectors. $\\it{d}$ is the dimension of the lattice in $\\mathbb{R}^n$.\nA lattice in 2D\rA lattice has many bases but some are more interesting than others. The goal of lattice reduction presented in the future sections is to find a qualitatively good basis composed of short and almost orthogonal vectors.\nIn the rest of this work, we will especially consider q-ary lattices described by a basis $\\bold{B}$, in which coefficients are taken modulo q. The membership of a vector to the lattice is then determined by $\\bold{x} \\text{ mod q}$.\nVolume of a lattice\r#\rIn each lattice, we can define the volume of the lattice as the volume of its fundamental parallellepiped (the area delimited by the basis vectors). This quantity is an invariant of the lattice and does not depend on the basis chosen. This means applying Gram-Schmidt orthogonalization to any basis will give us an orthogonal basis from which we can approximate the volume of the lattice as\n$$ Vol(\\Lambda) = \\prod_{i=0}^{d-1} \\lVert \\bold{b_i}^*\\rVert $$where $\\bold{b_i}*$ are the orthogonal Gram-Schmidt vectors. It is also important to remember that $Vol(\\Lambda) = |Det(\\bold{B})|$, where $\\bold{B}$ is the basis matrix.\nA lattice volume in 2D\rThis invariant is conceptually important because it tells us that not all basis vectors can be small at the same time.\nDual of a lattice\r#\rThe dual of a lattice $\\Lambda$ in $\\mathbb{R}^d$ is denoted $\\Lambda^*$ and is the lattice defined by vectors $\\bold{y}$ such that:\n$$\\langle \\bold{x}, \\bold{y}\\rangle \\in \\mathbb{Z}, \\forall \\bold{x} \\in \\Lambda$$We have that for any lattice described by basis $\\bold{B}$,\n$$\\Lambda(\\bold{B})^*=\\Lambda((\\bold{B}^{-1})^T) \\text{ and } Vol(\\bold{\\Lambda^*})=\\frac{1}{Vol(\\bold{\\Lambda})}$$\rMinima of a lattice\r#\rWe denote by $\\lambda_i(\\B)$ the $i$-th minimum of a lattice described by a basis B\nGaussian heuristic\r#\rThe gaussian heuristics predicts the number of lattice points inside any measurable body $\\mathcal{B} \\subset \\mathbb{R}^d$ is approximately $\\frac{Vol(\\mathcal{B})}{Vol(\\Lambda)}$. Applied to an euclidean d-ball, this would give that the length of the first vector is approximately\n$$\\lambda_1(\\Lambda) \\approx (\\frac{Vol(\\mathcal{B})}{Vol(\\Lambda)})^{\\frac{1}{d}} \\approx \\sqrt{\\frac{d}{2\\pi e}} Vol(\\Lambda)^{\\frac{1}{d}}$$\rRoot Hermite Factor\r#\rWe will next want to introduce a value called the root hermite factor. It is a measure used in lattice reduction theory to evaluate the quality of a reduced lattice basis. It is commonly used to assess the effectiveness of lattice reduction algorithms. It quantifies how much longer the shortest vector in a reduced lattice basis is, compared to the length of an ideal shortest vector, scaled by the lattice dimension. Formally we define it as\n$$\\lVert \\bold{b_1} \\rVert \\approx \\delta^d Vol(\\Lambda)^\\frac{1}{d}$$ for an d-dimensional lattice.\nThe closer $\\delta$ gets to 1, the better the reduction quality will be. This is of direct impact in our context, since we need to balance a trade-off between the quality of the output basis and the cost of running our lattice reduction algorithm. In fact, with the BKZ algorithm which has become the standard, a bigger block-size leads to a better quality of output basis but also a greater computational cost.\nThe Short Vector Problem\r#\rThe security of lattice based constructions rely on the fact that finding the shortest non-zero vector in a lattice is a hard problem. One usually considers an approximation version of the SVP problem. Given a basis \\(\\bold{B}\\) of a lattice, the approximate SVP problem is the problem of finding a short lattice vector \\(\\bold{v}\\) such that \\(0 \u0026lt; \\lVert \\bold{v} \\rVert \\leq \\gamma\\lambda_1(\\Lambda(\\bold{B}))\\) where \\(\\lambda_1\\) denotes the shortest nonzero vector length and \\(\\gamma\\) is the approximation factor.\nIt is actually believed that no polynomial time algorithm can approximate such a lattice problem within polynomial factors. Furthermore, it is also believed that no polynomial time quantum algorithm can approximate such a lattice problem within polynomial factors.\nLattice reduction algorithms\r#\rLattice reduction algorithms aim to transform a given basis of a lattice into a \u0026ldquo;reduced\u0026rdquo; basis, where the vectors are shorter and closer to being orthogonal. This reduction makes it easier to approximate solutions to hard lattice problems like the Shortest Vector Problem. In short, the quality of a basis can be improved to make the problem easier. The most prominent lattice reduction algorithms include:\nLLL Algorithm (Lenstra–Lenstra–Lovász): it produces a reduced basis in polynomial time, where the vectors are guaranteed to be within a known factor of the shortest vector, but doesn’t necessarily find the shortest vector, BKZ Algorithm (Block Korkine-Zolotarev): the BKZ algorithm is a generalization of the LLL algorithm and provides better reduction at the cost of higher computational complexity. By working in blocks of the lattice and applying LLL reduction to these blocks, BKZ achieves stronger approximations of the shortest vector, though it requires more computational resources. Sieving and Enumeration: algorithms that are designed to give exact solutions to the SVP problem. Security\r#\rThe foundational belief of security in new lattice-based primitives stems from one key finding. Ajtai’s theorem connected the hardness of certain average-case problems to the difficulty of worst-case problems in lattices. Specifically, Ajtai demonstrated that for the Short Integer Solution (SIS) problem which we will define later, the average-case instances are at least as hard as the worst-case instances of the Shortest Vector Problem (SVP) on lattices. This means that if one could efficiently solve random instances of SIS, then they could also solve the worst-case SVP, a problem believed to be intractable even for quantum computers. Ajtai’s theorem provides a strong security guarantee for lattice-based cryptographic schemes by grounding their security in the hardness of well-studied lattice problems like SVP.\n"},{"id":1,"href":"/docs/lattice-reduction/","title":"Lattice reduction","section":"Docs","content":"\rLattice reduction\r#\rLattice reduction algorithms such as LLL and BKZ make iterative local improvements to a basis. This means that the global cost can be seen as two-folds: how costly is it to make the local improvements, which corresponds to solving an exact SVP problem and how costly is the global behavior of the algorithm. This section focuses on the global behavior of lattice reduction algorithms, while the next section (Cost models) will focus about solving local improvements. Lattice reduction is the essential tool that allows to transform an inappropriate basis to solve the SIS problem into an appropriate one and thus its complexity is the bulk of the security estimate.\nUseful quantities in lattices\r#\rVolume\r#\rIn each lattice, we can define the volume of the lattice as the volume of its fundamental parallellepiped (the area delimited by the basis vectors). This quantity is an invariant of the lattice and does not depend on the basis chosen. This means that by applying Gram-Schmidt orthogonalization to any basis will give us an orthogonal basis from which we can approximate the volume of the lattice as\n$$ Vol(\\Lambda) = \\prod_{i=0}^{d-1} \\lVert \\bold{b_i}^*\\rVert $$where $\\bold{b_i}*$ are the orthogonal Gram-Schmidt vectors. It is also important to remember that $Vol(\\Lambda) = |Det(\\bold{B})|$, where $\\bold{B}$ is the basis matrix.\nThis invariant is conceptually important because it tells us that not all basis vectors can be small at the same time.\nGaussian heuristic\r#\rThe gaussian heuristics predicts the number of lattice points inside any measurable body $\\mathcal{B} \\subset \\mathbb{R}^d$ is approximately $\\frac{Vol(\\mathcal{B})}{Vol(\\Lambda)}$. Applied to an euclidean d-ball, this would give that the length of the first vector is approximately\n$$\\lambda_1(\\Lambda) \\approx (\\frac{Vol(\\mathcal{B})}{Vol(\\Lambda)})^{\\frac{1}{d}} \\approx \\sqrt{\\frac{d}{2\\pi e}} Vol(\\Lambda)^{\\frac{1}{d}}$$\rRoot Hermite Factor\r#\rWe will next want to introduce a value called the root hermite factor. It is a measure used in lattice reduction theory to evaluate the quality of a reduced lattice basis. It is commonly used to assess the effectiveness of lattice reduction algorithms. It quantifies how much longer the shortest vector in a reduced lattice basis is, compared to the length of an ideal shortest vector, scaled by the lattice dimension. Formally we define it as\n$$\\lVert \\bold{b_1} \\rVert \\approx \\delta^d Vol(\\Lambda)^\\frac{1}{d}$$ for an d-dimensional lattice.\nThe closer $\\delta$ gets to 1, the better the reduction quality will be. This is of direct impact in our context, since we need to balance a trade-off between the quality of the output basis and the cost of running our lattice reduction algorithm. In fact, with the BKZ algorithm which has become the standard, a bigger block-size leads to a better quality of output basis but also a greater computational cost.\nGeometric Series Assumption\r#\rThe geometric series assumption conceptually tells us that the Gram-Schmidt vectors log-length outputed by a lattice reducion algorithm will follow a line (see the graphs in the LLL subsection). We can formulate it as:\n$$\\lVert \\bold{b_i}^*\\rVert \\approx \\alpha^{i-1}\\lVert \\bold{b_1}\\rVert$$and in fact by combining it with the root hermite factor definition we can get a relation between the quality of the reduction and the slope of the GSA assumption as $\\alpha \\approx \\delta^{-2}$ leading to\n$$ \\lVert \\bold{b_i}^*\\rVert \\approx \\alpha^{i-1} \\delta^d Vol(\\Lambda)^\\frac{1}{d} = \\delta^{-1(i+1) + d} Vol(\\Lambda)^\\frac{1}{d}$$\rLLL algorithm\r#\rThe Lenstra-Lenstra-Lovász (LLL) algorithm is an efficient polynomial-time algorithm in lattice theory that finds a \u0026ldquo;nearly orthogonal\u0026rdquo; basis for a given lattice. It aims to transform any arbitrary basis of a lattice into a reduced basis where the basis vectors are short and close to orthogonal following two conditions:\nSize reduction: $$1 \\leq j \u003c i \\leq d\\colon \\left|\\mu_{i,j}\\right|\\leq 0.5 \\text{ for } \\mu_{i,j} =\\frac{\\langle\\mathbf{b}_i,\\mathbf{b}^*_j\\rangle}{\\langle\\mathbf{b}^*_j,\\mathbf{b}^*_j\\rangle}$$ Lovász condition: For $k=2,\u0026hellip;,d$ $$\\omega \\Vert \\mathbf{b}^*_{k-1}\\Vert^2 \\leq \\Vert \\mathbf{b}^*_k\\Vert^2+ \\mu_{k,k-1}^2\\Vert\\mathbf{b}^*_{k-1}\\Vert^2$$ We say the basis is LLL-reduced if there exists a parameter $\\omega \\in (0.25, 1)$.\nWhile there exists some theorems bounding the worst cases of lattice reduction algorithms, they tend to perform better in practice. Reasoning about the behaviors of such algorithms has therefore become a game of heuristics and approximations. Typically, the vectors that are outputed by the LLL algorithm are said to follow the geometric series assumption in their length. Again, this assumption tells us that the shape after lattice reduction is a line with a flatter slope as lattice reduction gets stronger. The goal of lattice reduction algorithm can therefore be interpreted seen by watching a graph of the log-length of vectors after reductions. The overall goal being to flatten the line, leading to a small basis.\nCost of LLL\r#\rThe theoretic bound on the quality of the LLL is $\\delta^d = (\\frac{4}{3})^\\frac{d-1}{4}$ leading to approximately $\\delta \\approx 1.075$. In practice, we get much better results on average, empirically about $\\delta \\approx 1.021$.\nIn terms of runtime, we will consider a heuristic bound (which better approximates empirical results) of $O(d^3 \\log^2(B))$.\nBKZ algorithm\r#\rThe Block Korkine-Zolotarev (BKZ) algorithm is a lattice reduction algorithm that generalizes the LLL algorithm to achieve stronger reduction properties. The BKZ algorithm is defined as a blockwise reduction algorithm that iteratively applies a form of lattice basis reduction to overlapping blocks of vectors within the basis. Assuming we have an SVP oracle, the BKZ algorithm is defined as follows:\nData: LLL-reduced basis B (pre-processed) and block size beta\rrepeat until no changes\rfor k in 0 to d-1\rLLL on local projected block [k, ..., k+beta-1]\rv \u0026lt;-- SVP-Oracle(local projected block[k, ..., k+beta-1])\rinsert v into B\rend\rA BKZ-$\\beta$ reduced basis satisfies, for $\\epsilon \u0026gt; 0$:\n$$\\lVert \\bold{b_0} \\rVert \\leq \\sqrt{(1 + \\epsilon) \\gamma_{\\beta}}^\\frac{d-1}{\\beta - 1} Vol(\\Lambda(\\bold{B}))$$$$\\gamma_\\beta = \\sup \\{ \\lambda_1(\\Lambda) | \\Lambda \\in \\mathbb{R}^\\beta, Vol(\\Lambda) = 1 \\}$$is the hermite constant.\nBy combining the gaussian heuristic and the definition of a BKZ-$\\beta$ reduced basis, we arrive again at the geometric assumption, which states that the length of reduced vectors follow a geometric series (which we can plot as a line as we did for LLL). This time however, it depends on the block-size chosen to run BKZ.\nWe can write\n$$\\log(\\lVert \\bold{b_i}^*\\rVert) = \\frac{d - 1 - 2i}{2}\\log(\\alpha_\\beta) + \\frac{1}{d}\\log(Vol(\\Lambda))$$where $\\alpha_\\beta$ is the slope under the geometric assumption that can be calculated from the gaussian assumption as\n$$\\alpha_\\beta = \\sqrt{\\frac{d}{2\\pi e}}^\\frac{2}{\\beta - 1}$$This estimate is reasonably accurate only if $d\\gg\\beta$ and $\\beta \u0026gt; 50$, which is why we will use fixed estimates for small dimensions.\nCost of BKZ\r#\rCosting BKZ means having a good idea of the impact of the block-size on the quality of our reduced basis. For this, we could either make the approximation $\\delta_\\beta \\approx \\sqrt \\alpha_\\beta$ or use the following limit defined in $$\\lim_{\\beta\\rightarrow\\infty}\\delta_\\beta = (\\frac{\\beta}{2\\pi e}(\\pi\\beta)^\\frac{1}{\\beta})^\\frac{1}{2(\\beta - 1)}$$ and write for SIS\n$$\\lVert \\bold{b_1} \\rVert \\approx \\delta_\\beta^{d-1} Vol(\\Lambda)^{\\frac{1}{d}}$$ Costing BKZ as a whole is complicated because we do not know how many tours we will have to run, which means we don\u0026rsquo;t really know in advance the number of SVP-Oracle calls we will have to make. Furthermore, many improvements on plain BKZ have been made when some techniques are used as a subroutine for the oracle (for example extreme pruning in the context of enumeration), which makes security estimates done via lattice reduction very sensitive to many factors. Also, local preprocessing techniques in a variant of BKZ known as progressive BKZ. To make our tool comparable to the lattice estimator by \u0026hellip;[insert], we will follow the same simplifying assumption and consider a consistent 8 tours of BKZ. This makes sense following experimental results that showed that most progress is made in the 7-9 first tours. We will then use: $$cost = \\tau \\cdot d \\cdot T_{SVP}$$ where the number of tours we do $\\tau$ is considered to be 8. The number of times the SVP oracle is called per tour, which is dimension of the lattice, is d The cost of the SVP oracle is $T_{SVP}$ BKZ 2.0 Tweaks\r#\rThe BKZ 2.0 algorithm introduces several key optimizations to improve the efficiency and effectiveness of the lattice reduction process. These enhancements address both the quality of the reduced basis and the computational overhead:\nExtreme Pruning:\nExtreme pruning focuses the SVP oracle\u0026rsquo;s efforts on the most promising branches of the search space, reducing unnecessary computations. This technique significantly accelerates the oracle’s performance without compromising the output quality. Adaptive Block Sizes:\nBKZ 2.0 dynamically adjusts the block size ($\\beta$) during execution. By using smaller block sizes initially and increasing them later, the algorithm finds a better trade-off between runtime and reduction quality. Improved Local Search:\nAdvanced local search algorithms enhance the process of finding short vectors within each block. These improvements reduce the time spent on individual blocks while maintaining or improving the overall basis quality. Experimental Impact:\nEmpirical studies show that BKZ 2.0 achieves better reduction quality than the original BKZ, especially for large dimensions. It also demonstrates improved scalability and efficiency in practice. The Lie of the GSA Assumption and Better Simulators (ZGSA)\r#\rThe Geometric Series Assumption (GSA), while widely used, does not perfectly capture the behavior of lattice reduction algorithms. This section highlights its limitations and introduces a more accurate model, ZGSA:\nWhy GSA Falls Short:\nThe GSA assumes that the Gram-Schmidt vector lengths decay geometrically. However, this assumption breaks down for smaller lattices or when dealing with structured lattices that deviate significantly from randomness. Introduction to ZGSA:\nThe Zero-Forced Geometric Series Assumption (ZGSA) refines the GSA by accounting for boundary effects and deviations observed in practical reductions. ZGSA provides a more precise representation of the reduced basis. Comparison with GSA:\nEmpirical results indicate that ZGSA predictions align more closely with observed reductions compared to GSA. A comparison of plots or metrics can demonstrate this improved accuracy. Applications of ZGSA:\nZGSA has been instrumental in refining security estimates for lattice-based cryptography, particularly in the context of parameter selection and attack simulations. The Dimension for Free (Ducas)\r#\rThe \u0026ldquo;dimension for free\u0026rdquo; concept introduced by Ducas reduces the cost of lattice reduction in high-dimensional lattices without sacrificing output quality. This approach is particularly relevant in practical implementations of lattice reduction algorithms.\nConcept Overview:\nThe \u0026ldquo;dimension for free\u0026rdquo; principle exploits structural redundancies in high-dimensional lattices, enabling efficient reductions with fewer computational resources. Impact on BKZ:\nBy incorporating the \u0026ldquo;dimension for free,\u0026rdquo; BKZ and BKZ 2.0 achieve improved runtime performance. This is especially beneficial in cryptographic applications requiring reductions in very high dimensions. Trade-offs and Limitations:\nWhile highly effective in many cases, the \u0026ldquo;dimension for free\u0026rdquo; may not yield significant improvements for small lattices or those lacking sufficient dimensionality for optimization. Practical Implications:\nThe \u0026ldquo;dimension for free\u0026rdquo; approach has become a cornerstone of efficient lattice reduction techniques. Its adoption in lattice-based cryptography has further solidified its role in modern security parameter design. "},{"id":2,"href":"/docs/cost-models/","title":"Cost Models","section":"Docs","content":"\rCost Models\r#\rGiven the simplified cost for BKZ behaviour that we consider $cost = \\tau \\cdot d \\cdot T_{SVP}$, we still need to define the cost of the SVP solver. Sieving and enumeration are the two common strategies to find the shortest non-zero vector in a lattice.\nSieving\r#\rSieving algorithms for SVP work by iteratively refining a large set of lattice vectors to obtain progressively shorter vectors until the shortest one is found. In its basic form, the sieving process starts by generating a large set of random lattice vectors, often called a \u0026ldquo;cloud.\u0026rdquo; Pairs of vectors are then combined (usually by adding or subtracting them) to produce shorter vectors, which are then added back to the cloud if they meet certain criteria. This process continues until the vectors in the cloud converge towards the shortest lattice vector. Modern sieving methods, like the GaussSieve or ListSieve, have been optimized to handle higher-dimensional lattices by limiting pairwise vector interactions, which reduces computational complexity. Sieving requires storing a large number of lattice vectors and, therefore, can be memory-intensive, especially as the lattice dimension grows.\nIn terms of cost, sieving algorithm can solve the SVP in a lattice of dimension d in $2^{O(k)}$ time but at the cost og a much higher memory usage. At the risk of being overly conservative, our estimator will disregard memory usage.\nThe following sieving estimates are available:\nName Reference Cost Regime BDGL-sieve todo $2^{0.292\\beta + 16.4}$ big $\\beta$ $2^{0.387\\beta + 16.4}$ small $\\beta$ classical Q-sieve todo $2^{0.265\\beta}$ quantum ADPS-sieve tedo $2^{0.292\\beta}$ classical BJG-sieve todo $2^{0.311\\beta}$ classical ChaLoy-sieve todo $2^{0.257\\beta}$ quantum Here are some plots to better visualize sieving costs.\nEnumeration\r#\rEnumeration algorithms systematically search through lattice points in a controlled way, typically by traversing lattice vectors within a fixed radius from the origin. They rely on a recursive process to explore potential candidate vectors within a \u0026ldquo;search region,\u0026rdquo; using techniques to prune paths that are unlikely to lead to the shortest vector. Enumeration is typically carried out with the help of a basis that has been reduced (made close to orthogonal), as this greatly improves efficiency by minimizing the number of candidate paths. Unlike sieving, enumeration methods are deterministic and guarantee finding the shortest vector by systematically exploring all feasible paths. The efficiency of enumeration depends strongly on the quality of the lattice basis. Preprocessing steps like BKZ (Block Korkine-Zolotarev) reduction can make enumeration significantly faster by transforming the basis to be more suitable for search. Enumeration is often practical for lower-dimensional lattices or when a high degree of accuracy is needed, but it tends to be less efficient than sieving in high dimensions due to its exponential complexity.\nIn terms of cost, enumeration can solve SVP in a lattice of dimension d in $2^{O(k \\log k)}$ time and space.\nThe following enumeration estimates are available:\nName Reference Cost Regime Lotus todo $2^{0.125\\beta\\log\\beta -0.755\\beta + 22.74}$ classical CheNgue-enum todo $2^{0.27\\beta\\log\\beta -1.019\\beta + 2.254}$ classical ABF-enum todo $2^{0.184\\beta\\log\\beta - 0.995\\beta + 22.25}$ small $\\beta$ $2^{0.125\\beta\\log\\beta-0.547\\beta+16.4}$ big $\\beta$ classical ABF-Q-enum todo $2^{0.0625\\beta\\log\\beta}$ quantum ABLR-enum todo $2^{0.184\\beta\\log\\beta - 1.077\\beta + 35.12}$ small $\\beta$ $2^{0.125\\beta\\log\\beta-0.655\\beta+31.84}$ big $\\beta$ classical Here are some plots to better visualize enumeration costs.\nMethod comparison\r#\rWe can see that sieving performs better overall when the dimension of the SVP problem get big, which is the expected behavior. For this reason, we encourage making security estimates with sieving as an underlying SVP solver.\n"},{"id":3,"href":"/docs/sis/","title":"SIS","section":"Docs","content":"\rShort Integer Solution problem (SIS)\r#\rLet us look at two different ways to define q-ary lattices. From \\(\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}\\)\n$$\\Lambda_q(\\bold{A^T}) = \\{\\bold{y} \\in \\mathbb{Z}^w : \\bold{y} = \\bold{A}^T\\bold{s} \\text{ mod q }\\text{ for some } \\bold{s} \\in \\mathbb{Z}^h\\}$$$$\\Lambda^T_q(\\bold{A}) = \\{\\bold{y} \\in \\mathbb{Z}^w : \\bold{A}\\bold{y} = \\bold{0} \\text{ mod q }\\}$$The first lattice is generally called the primal problem and relates to the LWE problem since finding a short vector in \\(\\Lambda_q(\\bold{A^T})\\) corresponds to solving the LWE problem. The second lattice is generally called the dual problem and relates to the SIS problem since finding a short vector in \\(\\Lambda^T_q(\\bold{A})\\) corresponds to solving the SIS problem.\nFormal definition\r#\rWe define \\(SIS(h, q, w, \\beta)\\) as follows:\nGiven \\(\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}\\) find the short vector \\(\\bold{s} \\in \\mathbb{Z}^w\\) where \\(0 \u0026lt; \\lVert s \\rVert_p \\leq \\beta\\).\nWe can note that the problem becomes trivial as soon as \\(\\beta \\geq q\\), no matter the norm used. In the following analysis, we will separate the use of the euclidean norm and the infinity norm. The two sections will describe the concrete ways of defining the security parameters, using the theory we built in the previous sections (lattice reduction and cost models).\nSIS and ISIS\r#\rSIS and LWE\r#\rL2 norm strategy\r#\rTODO check the notation between A and A^T\nFinding the optimal lattice shape for reduction\r#\rGiven a q-ary lattice \\(\\bold{A} \\in \\mathbb{Z}_q^{h\\times w}\\) and assuming q is prime (which will be the case in most applications), we can then say with high probability that the rows of $\\bold{A}$ are independent over $\\mathbb{Z}_q$ (we also assume w is bigger than n and not too close). As a result of this, the lattice $\\Lambda^T_q(\\bold{A})$ has $q^h$ points in $\\mathbb{Z}^w_q$. This leads to the volume or determinant of the matrix to be $Vol(\\Lambda^T_q(\\bold{A})) = q^{h}$. Using the gaussian heuristic from the lattice reduction section, we can express\n$$\\lambda_1(\\Lambda^T_q(\\bold{A})) \\approx q^\\frac{h}{w}\\sqrt{\\frac{w}{2\\pi e}}$$as an estimate of the lenght of the smallest vector.\n$$w = \\sqrt{\\frac{h\\log q}{\\delta}}$$ Indeed, for smaller m the lattice becomes too sparse and does not contain enough vectors to have small ones, and bigger m actually prevents lattice reduction to perform optimally.\nL-inf norm strategy\r#\rThe tradeoffs\r#\rThe search function\r#\rMatzov improvements on the dual attacks\r#\rKyber improvements\r#\r"},{"id":4,"href":"/docs/searching/","title":"Searching for the right parameters","section":"Docs","content":"\rFinding the right parameter set\r#\rWhat the search space looks like\r#\rWhat the estimator offers\r#\rSingle SIS instances\r#\rMultiple SIS instances\r#\r"},{"id":5,"href":"/docs/sis-variants/basis/","title":"Basis","section":"SIS variants","content":"\rBASIS\r#\rBasis rand\r#\rScheme\r#\rReduction\r#\rUsage\r#\rBasis struct\r#\rScheme\r#\rReduction\r#\rUSage\r#\rPower basis and link to Prisis\r#\r"},{"id":6,"href":"/docs/sis-variants/isisf/","title":"Isisf","section":"SIS variants","content":"\rISISf\r#\rScheme\r#\rReduction\r#\rUSage\r#\r"},{"id":7,"href":"/docs/sis-variants/kmsis/","title":"Kmsis","section":"SIS variants","content":"\rk-M-SIS\r#\rScheme\r#\rReduction\r#\rUSage\r#\rk-R-SIS\r#\rScheme\r#\rReduction\r#\rUSage\r#\r"},{"id":8,"href":"/docs/sis-variants/krisis/","title":"Krisis","section":"SIS variants","content":"\rkrISIS\r#\rScheme\r#\rReduction\r#\rUSage\r#\r"},{"id":9,"href":"/docs/sis-variants/ksis/","title":"Ksis","section":"SIS variants","content":"\rk-SIS\r#\rScheme\r#\rReduction\r#\rUSage\r#\r"},{"id":10,"href":"/docs/sis-variants/module-sis/","title":"Module Sis","section":"SIS variants","content":"\rModule-SIS\r#\rScheme\r#\rReduction\r#\rUSage\r#\r"},{"id":11,"href":"/docs/sis-variants/prisis/","title":"Prisis","section":"SIS variants","content":"\rPRISIS\r#\rScheme\r#\rReduction\r#\rUSage\r#\r"},{"id":12,"href":"/docs/sis-variants/ring-sis/","title":"Ring Sis","section":"SIS variants","content":"\rRing-SIS\r#\rScheme\r#\rReduction\r#\rUSage\r#\r"}]